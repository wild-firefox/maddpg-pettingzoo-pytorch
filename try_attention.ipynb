{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n",
      "tensor([[[-0.7812,  0.1937,  0.6831,  0.3020, -1.2855,  0.0128,  0.4752,\n",
      "          -0.6468, -1.7165, -0.4823,  2.3699, -2.2589, -1.0785,  1.7208,\n",
      "           0.0644, -0.3116,  0.4442, -0.9582,  0.9313,  0.1171, -0.9809,\n",
      "           0.9163, -0.3754,  0.0775, -0.4352,  0.1542, -0.5990, -1.6279,\n",
      "          -0.2050, -1.2688,  0.2543,  1.2175,  0.2484, -1.2445,  1.2138,\n",
      "          -2.5827,  2.4274, -0.3891,  0.6663,  0.2754, -2.1325, -1.4978,\n",
      "          -0.3794, -1.5681,  0.9710,  0.7007,  0.4299,  0.1714, -1.0776,\n",
      "          -0.9400, -0.4812, -0.7455,  1.3695, -0.4955, -0.8966,  0.1539,\n",
      "           1.9773, -1.5391,  1.3548, -1.4211,  0.0226,  1.5352, -1.3459,\n",
      "           0.1566]]])\n",
      "tensor([[-0.5906, -2.0971, -0.1380,  ..., -1.6176, -0.3098, -0.2979],\n",
      "        [ 0.3817, -0.4182,  0.0311,  ..., -0.1159, -0.1287, -0.0273],\n",
      "        [ 0.0532,  0.0124,  1.1253,  ..., -1.0223,  0.0213,  1.3701],\n",
      "        ...,\n",
      "        [-1.0162, -0.8913,  1.8390,  ...,  1.0211,  0.5831,  1.8436],\n",
      "        [-1.1551, -1.8005,  1.2110,  ...,  0.3923,  1.0775,  1.0203],\n",
      "        [ 0.6347,  0.5431,  1.7846,  ...,  0.8945,  1.6501,  0.2304]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class Attention(nn.Module): \n",
    "    def __init__(self, dmodel, dk): \n",
    "        super(Attention, self).__init__() \n",
    "        self.dk = dk # 键的维度\n",
    "        self.W = nn.Linear(dmodel, dk) #查询的线性层\n",
    "        self.V = nn.Linear(dk, dk)  # 调整输入维度 值的线性层\n",
    "        self.a = nn.Linear(dmodel, 1) #注意力的线性层\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        a = self.a(Q)\n",
    "        a = torch.tanh(a + self.W(Q) + K)  # 确保维度匹配\n",
    "        a = self.V(a)\n",
    "        a = torch.softmax(a, dim=-1)\n",
    "        return a * V\n",
    "    \n",
    "# 使用 Attention 模块\n",
    "attention = Attention(dmodel=64, dk=32) \n",
    "Q = torch.randn(1, 1, 64)  #(1,1,64) 1个样本 1个查询 64个维度 1*1*64\n",
    "K = torch.randn(1, 32, 32)  #(1,32,32) 1个样本 32个键 32个维度 1*32*32\n",
    "V = torch.randn(1, 32, 32)\n",
    "\n",
    "output = attention(Q, K, V) \n",
    "print(output.shape) \n",
    "#print(output) \n",
    "print(Q)\n",
    "print(K[0])\n",
    "\n",
    "\n",
    "# torch.Size([1, 1, 32]) ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "torch.Size([12, 1])\n",
      "1\n",
      "tensor([[ 3.7421e-01,  4.7305e-02,  2.4503e-03, -1.7902e-03,  6.8952e-02,\n",
      "          1.5911e-01, -1.1148e-01,  3.2580e-01,  2.3623e-02,  1.6507e-01,\n",
      "          2.8500e-02,  1.7523e-02,  1.3803e-01,  2.0690e-01,  2.4201e-02,\n",
      "         -1.1153e-01,  8.0610e-03,  1.5245e-01, -7.2237e-02,  8.8497e-02,\n",
      "         -1.5001e-01, -1.4662e-01,  4.8354e-01, -6.8436e-02,  2.4036e-01,\n",
      "          1.2587e-01, -2.9741e-01, -1.9121e-02, -3.4182e-01, -3.6344e-02,\n",
      "         -1.1900e-01,  1.6724e-01,  3.6068e-01, -7.9976e-02,  1.6924e-01,\n",
      "         -5.1975e-02, -1.9620e-01, -2.6547e-03,  1.1183e-01, -3.5658e-02,\n",
      "          4.1525e-01,  2.4581e-01, -4.7811e-01,  1.6771e-01,  1.8765e-01,\n",
      "          3.5561e-02,  2.0565e-01,  1.7503e-01,  6.9023e-02, -2.1510e-01,\n",
      "         -3.2191e-01, -2.7941e-01,  1.9586e-01,  1.6022e-01,  9.0823e-02,\n",
      "         -1.2894e-01, -2.6922e-01,  9.0844e-02,  1.4083e-01, -2.0077e-01,\n",
      "          5.7030e-02,  8.1929e-02, -6.1835e-02,  1.0544e-01],\n",
      "        [ 3.7658e-01,  5.0024e-02, -2.2891e-01,  2.3543e-02,  1.0575e-01,\n",
      "          1.0621e-01, -1.9568e-01,  3.2739e-01,  6.2386e-02,  1.3057e-01,\n",
      "          8.9980e-02,  9.2943e-02,  3.0694e-01,  2.3826e-01,  1.1554e-01,\n",
      "         -1.7533e-01,  9.7901e-03,  1.2007e-01, -2.4041e-02,  5.6766e-02,\n",
      "         -1.0122e-01, -6.9482e-02,  4.2893e-01, -3.0429e-02,  3.3788e-01,\n",
      "          2.4443e-01, -3.6491e-01, -1.2102e-01, -4.2655e-01, -1.9290e-02,\n",
      "         -1.6379e-01,  1.8387e-01,  2.6023e-01, -2.3589e-02,  1.5253e-01,\n",
      "         -9.5313e-02, -1.2560e-01, -2.0970e-02,  2.0722e-01,  7.0344e-02,\n",
      "          5.4457e-01,  1.1706e-01, -5.4091e-01,  1.4341e-01,  9.3948e-02,\n",
      "          9.7617e-02,  2.8248e-01,  1.0999e-01,  6.0252e-02, -2.0951e-01,\n",
      "         -2.9871e-01, -2.4308e-01,  2.4444e-01,  1.7182e-01,  2.2603e-01,\n",
      "         -1.1472e-01, -2.9533e-01, -3.6363e-02,  6.1169e-02, -3.2665e-02,\n",
      "          1.3076e-02,  1.4831e-01, -8.8879e-02, -8.1423e-03],\n",
      "        [ 3.8552e-01,  2.6630e-02,  5.1600e-02, -5.2339e-03,  3.6689e-02,\n",
      "          9.1157e-02, -6.9972e-02,  2.9513e-01,  8.9332e-02,  1.5979e-01,\n",
      "          1.4258e-01, -6.9746e-02,  2.1201e-01,  2.3889e-01, -5.0016e-02,\n",
      "         -9.2905e-02,  8.5909e-03,  1.3947e-01, -3.2713e-03, -2.1828e-02,\n",
      "         -1.1178e-01, -1.3531e-01,  3.9545e-01, -6.9543e-02,  1.4967e-01,\n",
      "          1.3148e-01, -2.9673e-01,  3.7461e-02, -3.5599e-01, -1.2174e-01,\n",
      "         -1.0330e-01,  1.1663e-01,  3.7430e-01, -5.5210e-02,  2.0729e-01,\n",
      "          6.3373e-02, -8.8411e-02, -6.3225e-02,  2.6785e-03, -9.6480e-02,\n",
      "          4.4155e-01,  2.5311e-01, -4.5725e-01,  1.1552e-01,  1.7260e-01,\n",
      "         -1.1632e-02,  8.2563e-02,  9.2462e-02,  8.5152e-02, -2.4328e-01,\n",
      "         -2.8435e-01, -1.8790e-01,  1.5548e-01,  1.6295e-01,  3.9326e-02,\n",
      "         -2.2739e-01, -2.0555e-01,  1.2270e-01,  2.0045e-01, -1.3884e-01,\n",
      "          2.3926e-02,  2.0643e-01, -2.4054e-02,  1.2911e-01],\n",
      "        [ 4.2087e-01,  4.1271e-02, -3.7992e-02,  5.6348e-02,  5.1114e-02,\n",
      "          4.5123e-02, -7.2758e-02,  3.1932e-01,  5.2185e-02,  1.4013e-01,\n",
      "          6.6667e-02, -1.2149e-02,  2.5602e-01,  1.7746e-01, -4.3208e-02,\n",
      "         -1.3507e-01, -2.9570e-02,  2.1424e-01, -2.0669e-02,  8.5294e-03,\n",
      "         -7.6525e-02, -8.7690e-02,  4.5064e-01, -8.4053e-03,  2.4623e-01,\n",
      "          1.0714e-01, -3.4966e-01,  1.4199e-02, -2.9863e-01, -8.8328e-02,\n",
      "         -1.1914e-01,  2.2059e-01,  3.0435e-01, -1.7253e-01,  2.1237e-01,\n",
      "         -9.7731e-02, -1.1178e-01, -5.7360e-02,  9.8084e-02,  8.8640e-03,\n",
      "          4.6409e-01,  2.5563e-01, -4.7423e-01,  8.2827e-02,  1.7840e-01,\n",
      "          1.0132e-02,  1.3924e-01,  1.5260e-01,  8.0349e-02, -2.0995e-01,\n",
      "         -1.9666e-01, -2.0878e-01,  6.4266e-02,  2.0937e-01,  1.2846e-01,\n",
      "         -1.6567e-01, -2.8593e-01,  1.3638e-01,  9.2040e-02, -7.8893e-02,\n",
      "         -1.4246e-02,  1.7518e-01, -5.4842e-02,  7.1478e-02],\n",
      "        [ 4.1563e-01,  2.4535e-02, -2.1761e-01, -1.7748e-01,  5.1978e-02,\n",
      "          9.1245e-02, -1.0134e-01,  2.4934e-01,  1.0204e-01,  7.0894e-02,\n",
      "          2.1058e-01,  5.4574e-02,  2.2634e-01,  3.3350e-01,  9.6868e-02,\n",
      "         -7.1571e-02,  4.0513e-02,  9.4664e-02, -2.0533e-02,  5.9374e-02,\n",
      "         -4.0986e-02,  2.6153e-03,  1.4676e-01,  4.8304e-02,  2.5978e-01,\n",
      "          2.2430e-01, -3.5502e-01, -1.4355e-01, -5.1927e-01, -1.2312e-01,\n",
      "         -2.7164e-02,  7.8583e-02,  1.9687e-01,  5.3238e-02,  2.5315e-01,\n",
      "         -6.6858e-02,  5.8641e-02,  6.3495e-02,  1.7595e-01,  1.1578e-01,\n",
      "          4.4619e-01,  1.7339e-01, -5.0421e-01,  1.6427e-01,  1.6037e-01,\n",
      "          2.9845e-02,  2.6452e-01,  1.4162e-01,  1.8532e-01, -2.9652e-01,\n",
      "         -3.5264e-01, -1.6369e-01,  1.3530e-01,  1.2585e-01,  2.7547e-01,\n",
      "         -2.1311e-01, -1.5485e-01, -4.8980e-03,  1.1283e-01, -1.1869e-01,\n",
      "         -2.7321e-03,  1.6533e-01, -1.2598e-01, -1.3980e-01],\n",
      "        [ 3.8654e-01,  1.3049e-02,  3.4992e-02, -9.7621e-02,  9.5578e-02,\n",
      "          1.4366e-01, -8.9994e-02,  2.9733e-01,  6.1718e-02,  1.3184e-01,\n",
      "          1.9331e-01,  5.1499e-02,  7.8288e-02,  2.2208e-01,  1.9691e-02,\n",
      "         -1.6806e-01,  1.3185e-02,  1.7471e-01, -7.2262e-02,  3.3013e-02,\n",
      "         -1.2426e-01, -1.4770e-01,  4.2425e-01, -3.2459e-02,  2.3744e-01,\n",
      "          1.6093e-01, -2.6957e-01, -1.1826e-01, -3.4049e-01, -5.7039e-02,\n",
      "         -9.4733e-02,  1.2298e-01,  3.8689e-01,  3.5659e-02,  2.2087e-01,\n",
      "         -1.5148e-03, -1.7236e-01,  2.8596e-02,  6.6166e-02, -1.3043e-01,\n",
      "          4.7793e-01,  1.7187e-01, -4.6038e-01,  1.9134e-01,  2.7852e-01,\n",
      "          6.1530e-02,  1.8298e-01,  1.4375e-01,  6.2690e-02, -1.6792e-01,\n",
      "         -3.0137e-01, -2.7293e-01,  2.5169e-01,  8.9886e-02,  2.1522e-02,\n",
      "         -2.2829e-01, -2.1659e-01,  4.0652e-02,  2.0073e-01, -2.1858e-01,\n",
      "          3.9766e-02,  1.5262e-01, -5.5053e-02,  6.9060e-02],\n",
      "        [ 3.8924e-01,  5.8152e-02, -1.8165e-02,  2.3887e-02, -2.4625e-02,\n",
      "          9.7413e-02, -7.6765e-02,  2.9088e-01,  8.4818e-02,  1.5713e-01,\n",
      "         -2.6939e-02, -5.1142e-02,  2.3761e-01,  2.2048e-01,  6.9481e-02,\n",
      "         -1.8503e-02, -2.4933e-02,  6.5321e-02, -5.7195e-02,  6.5645e-02,\n",
      "         -6.9247e-02, -6.4582e-02,  3.5944e-01, -2.7634e-02,  1.6530e-01,\n",
      "          1.1485e-01, -3.4943e-01,  9.8451e-02, -4.3325e-01, -3.6218e-02,\n",
      "         -6.3220e-02,  1.3738e-01,  2.6545e-01, -2.1009e-01,  1.5777e-01,\n",
      "         -9.1504e-02, -5.9366e-02, -4.8823e-02,  1.1661e-01,  8.4317e-02,\n",
      "          3.1847e-01,  3.4974e-01, -5.1344e-01,  1.2761e-01,  1.3596e-01,\n",
      "          2.0157e-02,  1.9648e-01,  2.0604e-01,  1.5587e-01, -3.0771e-01,\n",
      "         -3.8969e-01, -1.8519e-01,  3.2242e-02,  2.4064e-01,  2.2743e-01,\n",
      "         -1.1479e-01, -2.5088e-01,  1.3776e-01,  1.0788e-01, -1.1551e-01,\n",
      "          5.6492e-02,  1.3963e-01, -1.0901e-01,  5.9104e-02],\n",
      "        [ 4.2004e-01,  5.7933e-02, -3.7215e-02,  5.0573e-02, -7.7596e-03,\n",
      "          7.6061e-02, -6.9643e-02,  3.3045e-01,  2.2497e-02,  1.7737e-01,\n",
      "         -1.8081e-02, -1.1217e-01,  2.7980e-01,  2.1361e-01, -6.3502e-02,\n",
      "         -4.0337e-02, -1.6430e-02,  1.9206e-01,  6.7838e-03,  5.2858e-02,\n",
      "         -8.7818e-02, -1.1395e-01,  3.7831e-01, -7.9812e-02,  2.5128e-01,\n",
      "          7.4336e-02, -3.4281e-01,  1.1365e-01, -4.0340e-01, -1.9199e-01,\n",
      "         -1.1316e-01,  1.4389e-01,  3.1071e-01, -1.5032e-01,  1.8157e-01,\n",
      "         -6.0128e-02, -8.5805e-02, -8.4022e-02,  8.3662e-02,  3.4423e-02,\n",
      "          3.7129e-01,  2.9098e-01, -4.6800e-01,  7.9425e-02,  1.2983e-01,\n",
      "         -1.0195e-01,  1.5491e-01,  1.6780e-01,  1.3190e-01, -3.1523e-01,\n",
      "         -2.2061e-01, -1.8750e-01, -1.6820e-02,  2.5642e-01,  1.5092e-01,\n",
      "         -1.2669e-01, -2.8396e-01,  1.6979e-01,  9.4507e-02, -8.5562e-02,\n",
      "          2.5036e-02,  1.2992e-01, -1.1157e-02,  7.8776e-02],\n",
      "        [ 3.9836e-01,  4.3075e-02, -4.1099e-02, -8.7179e-03,  3.3797e-02,\n",
      "          8.6127e-02, -9.0071e-02,  2.6499e-01,  7.4645e-02,  1.4069e-01,\n",
      "          9.3599e-02, -6.0060e-02,  2.6440e-01,  2.6281e-01, -2.2011e-02,\n",
      "         -6.6702e-02,  3.8912e-02,  1.3570e-01,  2.0280e-02,  2.8238e-02,\n",
      "         -1.0808e-01, -4.8517e-02,  3.4670e-01, -9.0889e-02,  1.8911e-01,\n",
      "          1.2322e-01, -3.4025e-01,  3.1523e-02, -3.6881e-01, -8.7365e-02,\n",
      "         -1.1840e-01,  1.8014e-01,  3.4659e-01, -1.0805e-01,  1.8847e-01,\n",
      "         -2.4956e-02, -6.6929e-02, -3.9569e-02,  4.1022e-02,  2.0428e-02,\n",
      "          4.1195e-01,  3.1657e-01, -5.0841e-01,  8.8468e-02,  1.0037e-01,\n",
      "         -1.4397e-02,  1.0291e-01,  1.0779e-01,  8.3237e-02, -3.1607e-01,\n",
      "         -3.4460e-01, -1.9386e-01,  1.4447e-01,  1.9536e-01,  1.5387e-01,\n",
      "         -1.7341e-01, -1.9851e-01,  1.4411e-01,  1.7500e-01, -1.2065e-01,\n",
      "          1.9488e-02,  1.2949e-01, -1.4518e-02,  7.1196e-02],\n",
      "        [ 3.8802e-01,  6.5347e-02, -4.8477e-02,  3.9209e-02,  1.1707e-02,\n",
      "          1.1652e-01, -9.4445e-02,  3.3135e-01,  5.4462e-02,  1.5712e-01,\n",
      "          1.9739e-02, -2.8490e-02,  2.3276e-01,  2.1753e-01,  2.0018e-02,\n",
      "         -6.5892e-02, -2.4871e-02,  1.2139e-01, -6.6972e-02,  4.4623e-02,\n",
      "         -1.1013e-01, -1.4178e-01,  4.3875e-01, -4.3854e-03,  1.8791e-01,\n",
      "          1.3364e-01, -3.4690e-01,  5.8643e-02, -3.7844e-01, -9.4994e-02,\n",
      "         -1.1193e-01,  1.4358e-01,  2.6328e-01, -1.7923e-01,  1.7972e-01,\n",
      "         -2.5581e-02, -7.9780e-02, -6.6555e-02,  1.3033e-01,  2.0945e-02,\n",
      "          4.1753e-01,  2.3488e-01, -4.7476e-01,  1.2210e-01,  1.4268e-01,\n",
      "          2.3747e-02,  2.0510e-01,  1.6392e-01,  1.1697e-01, -2.3003e-01,\n",
      "         -2.4889e-01, -1.6858e-01,  7.0222e-02,  2.3696e-01,  1.5111e-01,\n",
      "         -1.3516e-01, -2.9091e-01,  8.5944e-02,  6.3975e-02, -9.1527e-02,\n",
      "          4.4352e-03,  1.6729e-01, -8.5715e-02,  7.4373e-02],\n",
      "        [ 3.8768e-01,  6.8489e-02, -3.7118e-03,  6.1017e-02,  9.9582e-02,\n",
      "          1.5154e-01, -1.0816e-01,  3.9006e-01, -2.6191e-02,  1.8885e-01,\n",
      "         -1.3519e-02,  2.6530e-02,  1.3398e-01,  1.9254e-01, -1.3290e-02,\n",
      "         -1.7087e-01,  1.0143e-02,  1.9724e-01, -1.0903e-01,  1.0040e-01,\n",
      "         -1.6500e-01, -2.1370e-01,  5.4897e-01, -7.7183e-02,  2.8246e-01,\n",
      "          8.3824e-02, -2.9321e-01,  1.5889e-02, -3.0068e-01, -5.6291e-02,\n",
      "         -1.2856e-01,  1.7381e-01,  3.9204e-01, -7.3298e-02,  1.4522e-01,\n",
      "         -2.0929e-02, -2.2489e-01, -2.6572e-02,  1.3111e-01, -7.3562e-02,\n",
      "          4.0709e-01,  1.8809e-01, -4.3260e-01,  1.7288e-01,  2.1834e-01,\n",
      "         -6.0111e-03,  2.1058e-01,  1.9949e-01,  6.3944e-02, -1.7139e-01,\n",
      "         -2.3643e-01, -2.8592e-01,  1.7339e-01,  1.8411e-01,  3.6993e-02,\n",
      "         -9.8956e-02, -3.1819e-01,  9.6136e-02,  9.2699e-02, -2.0339e-01,\n",
      "          5.0560e-02,  9.3483e-02, -4.0553e-02,  1.6463e-01],\n",
      "        [ 3.9933e-01,  4.7766e-02, -1.4683e-01, -2.0178e-02,  8.7706e-02,\n",
      "          1.2712e-01, -1.2991e-01,  3.6296e-01,  2.7058e-02,  1.4473e-01,\n",
      "          5.5421e-02,  6.8513e-02,  1.9678e-01,  2.3651e-01,  8.2345e-02,\n",
      "         -1.5040e-01, -6.7540e-03,  1.3477e-01, -8.6206e-02,  9.2523e-02,\n",
      "         -9.4123e-02, -1.3373e-01,  3.9667e-01, -3.5472e-03,  3.1254e-01,\n",
      "          1.8311e-01, -3.2425e-01, -7.7701e-02, -4.4103e-01, -7.8183e-02,\n",
      "         -8.4265e-02,  1.0501e-01,  2.7167e-01,  6.9543e-03,  1.7145e-01,\n",
      "         -5.9190e-02, -1.1698e-01,  3.6032e-03,  2.0628e-01,  2.9678e-02,\n",
      "          4.4422e-01,  1.1886e-01, -4.7306e-01,  1.9200e-01,  1.9716e-01,\n",
      "          3.0745e-02,  3.0549e-01,  1.9381e-01,  1.3302e-01, -2.0005e-01,\n",
      "         -2.7550e-01, -2.4281e-01,  1.5169e-01,  1.6439e-01,  1.7148e-01,\n",
      "         -1.2008e-01, -2.9223e-01,  4.6167e-05,  5.9890e-02, -1.2432e-01,\n",
      "          3.8789e-02,  1.4654e-01, -1.1794e-01,  1.2930e-02]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.query = nn.Linear(in_dim, hidden_dim) #查询\n",
    "        self.key = nn.Linear(in_dim, hidden_dim)\n",
    "        self.value = nn.Linear(in_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        公式为：Attention(Q, K, V) = softmax(Q*K^T/sqrt(d_k)) * V\n",
    "        Q: 查询\n",
    "        K: 键\n",
    "        V: 值\n",
    "        d_k: 键的维度 这里用hidden_dim表示 即:K.size(-1)\n",
    "        对张量 K 进行转置（transpose）。具体来说，这个操作会将张量 K 的第0维和第1维进行交换\n",
    "        '''\n",
    "        Q = self.query(x) \n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(torch.tensor(K.size(-1), dtype=torch.float32))\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attended_values = torch.matmul(attn_probs, V)\n",
    "        return attended_values\n",
    "\n",
    "class MLPNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim_1=256, hidden_dim_2=128, attention_dim=64):\n",
    "        super(MLPNetworkWithAttention, self).__init__()\n",
    "        self.attention = Attention(in_dim, attention_dim)\n",
    "        self.fc1 = torch.nn.Linear(attention_dim, hidden_dim_1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim_2, out_dim)\n",
    "        \n",
    "        # 根据计算增益\n",
    "        gain1 = nn.init.calculate_gain('relu')\n",
    "        # Xavier均匀分布初始化\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight, gain=gain1)\n",
    "        # 初始化参数\n",
    "        self.fc1.bias.data.fill_(0.01)\n",
    "        self.fc2.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, x): #这里的x是状态加动作\n",
    "        x = self.attention(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 创建带有注意力机制的critic\n",
    "global_obs_dim = 128  # 示例输入维度\n",
    "critic = MLPNetworkWithAttention(global_obs_dim, 1)\n",
    "#print(critic)\n",
    "query = nn.Linear(128, 1)\n",
    "x = torch.randn(12, 128)\n",
    "Q = query(x)\n",
    "print(Q.transpose(0, 1).size())\n",
    "print(Q.size()) #.size表示维度\n",
    "print(Q.size(-1))\n",
    "an = Attention(128, 64)\n",
    "output = an(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiAgentAttention(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(MultiAgentAttention, self).__init__()\n",
    "        self.query = nn.Linear(in_dim, hidden_dim)\n",
    "        self.key = nn.Linear(in_dim, hidden_dim)\n",
    "        self.value = nn.Linear(in_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, agent_states):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            agent_states (list of tensors): List of states for each agent\n",
    "        Outputs:\n",
    "            attended_values (tensor): Attention-weighted values for each agent\n",
    "        \"\"\"\n",
    "        Q = [self.query(state) for state in agent_states]\n",
    "        K = [self.key(state) for state in agent_states]\n",
    "        V = [self.value(state) for state in agent_states]\n",
    "        \n",
    "        Q = torch.stack(Q)\n",
    "        K = torch.stack(K)\n",
    "        V = torch.stack(V)\n",
    "        \n",
    "        attn_scores = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(torch.tensor(K.size(-1), dtype=torch.float32))\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attended_values = torch.matmul(attn_probs, V)\n",
    "        \n",
    "        return attended_values\n",
    "\n",
    "class MultiAgentMLPNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim_1=256, hidden_dim_2=128, attention_dim=64, ):\n",
    "        super(MultiAgentMLPNetworkWithAttention, self).__init__()\n",
    "        self.attention = MultiAgentAttention(in_dim, attention_dim)\n",
    "        self.fc1 = torch.nn.Linear(attention_dim, hidden_dim_1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim_2, out_dim)\n",
    "        \n",
    "        # 根据计算增益\n",
    "        gain1 = nn.init.calculate_gain('relu')\n",
    "        # Xavier均匀分布初始化\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight, gain=gain1)\n",
    "        # 初始化参数\n",
    "        self.fc1.bias.data.fill_(0.01)\n",
    "        self.fc2.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, agent_states):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            agent_states (list of tensors): List of states for each agent\n",
    "        Outputs:\n",
    "            outputs (list of tensors): Outputs for each agent\n",
    "        \"\"\"\n",
    "        attended_values = self.attention(agent_states)\n",
    "        outputs = []\n",
    "        for value in attended_values:\n",
    "            x = F.relu(self.fc1(value))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            output = self.fc3(x)\n",
    "            outputs.append(output)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  7,  8,  9, 13, 14, 17, 18],\n",
      "        [ 4,  5,  6, 10, 11, 12, 15, 16, 19, 20]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设 state_list 包含两个张量，每个张量的形状为 (batch_size, state_dim)\n",
    "state_list = [\n",
    "    torch.tensor([[1, 2, 3], [4, 5, 6]]),  # 形状为 (2, 3)\n",
    "    torch.tensor([[7, 8, 9], [10, 11, 12]])  # 形状为 (2, 3)\n",
    "]\n",
    "\n",
    "# 假设 act_list 包含两个张量，每个张量的形状为 (batch_size, action_dim)\n",
    "act_list = [\n",
    "    torch.tensor([[13, 14], [15, 16]]),  # 形状为 (2, 2)\n",
    "    torch.tensor([[17, 18], [19, 20]])  # 形状为 (2, 2)\n",
    "]\n",
    "x = torch.cat(state_list + act_list, 1) #按列拼接\n",
    "print(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
