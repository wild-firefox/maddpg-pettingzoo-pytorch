{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 根据MAT论文解读 根据自注意力的方式实现拟合Q值函数 -- 实际上是使用了Encoder 来实现Q值函数的拟合\n",
    "import torch\n",
    "import torch.nn as nn    \n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init(module, weight_init, bias_init, gain=1):\n",
    "    weight_init(module.weight.data, gain=gain)\n",
    "    if module.bias is not None:\n",
    "        bias_init(module.bias.data)\n",
    "    return module\n",
    "\n",
    "def init_(m, gain=0.01, activate=False): # 初始化权重 orthogonal_表示正交初始化\n",
    "    if activate:\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "    return init(m, nn.init.orthogonal_, lambda x: nn.init.constant_(x, 0), gain=gain)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head, n_agent, masked=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        assert n_embd % n_head == 0\n",
    "        self.masked = masked\n",
    "        self.n_head = n_head\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = init_(nn.Linear(n_embd, n_embd))\n",
    "        self.query = init_(nn.Linear(n_embd, n_embd))\n",
    "        self.value = init_(nn.Linear(n_embd, n_embd))\n",
    "        # output projection\n",
    "        self.proj = init_(nn.Linear(n_embd, n_embd))\n",
    "        # if self.masked:\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(n_agent + 1, n_agent + 1))\n",
    "                             .view(1, 1, n_agent + 1, n_agent + 1))\n",
    "\n",
    "        self.att_bp = None\n",
    "\n",
    "    def forward(self, key, value, query):\n",
    "        B, L, D = query.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(key).view(B, L, self.n_head, D // self.n_head).transpose(1, 2)  # (B, nh, L, hs)\n",
    "        q = self.query(query).view(B, L, self.n_head, D // self.n_head).transpose(1, 2)  # (B, nh, L, hs)\n",
    "        v = self.value(value).view(B, L, self.n_head, D // self.n_head).transpose(1, 2)  # (B, nh, L, hs)\n",
    "\n",
    "        # causal attention: (B, nh, L, hs) x (B, nh, hs, L) -> (B, nh, L, L)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "\n",
    "        # self.att_bp = F.softmax(att, dim=-1)\n",
    "\n",
    "        if self.masked:\n",
    "            att = att.masked_fill(self.mask[:, :, :L, :L] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        y = att @ v  # (B, nh, L, L) x (B, nh, L, hs) -> (B, nh, L, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, L, D)  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.proj(y)\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
