{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n",
      "tensor([[[-0.7812,  0.1937,  0.6831,  0.3020, -1.2855,  0.0128,  0.4752,\n",
      "          -0.6468, -1.7165, -0.4823,  2.3699, -2.2589, -1.0785,  1.7208,\n",
      "           0.0644, -0.3116,  0.4442, -0.9582,  0.9313,  0.1171, -0.9809,\n",
      "           0.9163, -0.3754,  0.0775, -0.4352,  0.1542, -0.5990, -1.6279,\n",
      "          -0.2050, -1.2688,  0.2543,  1.2175,  0.2484, -1.2445,  1.2138,\n",
      "          -2.5827,  2.4274, -0.3891,  0.6663,  0.2754, -2.1325, -1.4978,\n",
      "          -0.3794, -1.5681,  0.9710,  0.7007,  0.4299,  0.1714, -1.0776,\n",
      "          -0.9400, -0.4812, -0.7455,  1.3695, -0.4955, -0.8966,  0.1539,\n",
      "           1.9773, -1.5391,  1.3548, -1.4211,  0.0226,  1.5352, -1.3459,\n",
      "           0.1566]]])\n",
      "tensor([[-0.5906, -2.0971, -0.1380,  ..., -1.6176, -0.3098, -0.2979],\n",
      "        [ 0.3817, -0.4182,  0.0311,  ..., -0.1159, -0.1287, -0.0273],\n",
      "        [ 0.0532,  0.0124,  1.1253,  ..., -1.0223,  0.0213,  1.3701],\n",
      "        ...,\n",
      "        [-1.0162, -0.8913,  1.8390,  ...,  1.0211,  0.5831,  1.8436],\n",
      "        [-1.1551, -1.8005,  1.2110,  ...,  0.3923,  1.0775,  1.0203],\n",
      "        [ 0.6347,  0.5431,  1.7846,  ...,  0.8945,  1.6501,  0.2304]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class Attention(nn.Module): \n",
    "    def __init__(self, dmodel, dk): \n",
    "        super(Attention, self).__init__() \n",
    "        self.dk = dk # 键的维度\n",
    "        self.W = nn.Linear(dmodel, dk) #查询的线性层\n",
    "        self.V = nn.Linear(dk, dk)  # 调整输入维度 值的线性层\n",
    "        self.a = nn.Linear(dmodel, 1) #注意力的线性层\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        a = self.a(Q)\n",
    "        a = torch.tanh(a + self.W(Q) + K)  # 确保维度匹配\n",
    "        a = self.V(a)\n",
    "        a = torch.softmax(a, dim=-1)\n",
    "        return a * V\n",
    "    \n",
    "# 使用 Attention 模块\n",
    "attention = Attention(dmodel=64, dk=32) \n",
    "Q = torch.randn(1, 1, 64)  #(1,1,64) 1个样本 1个查询 64个维度 1*1*64\n",
    "K = torch.randn(1, 32, 32)  #(1,32,32) 1个样本 32个键 32个维度 1*32*32\n",
    "V = torch.randn(1, 32, 32)\n",
    "\n",
    "output = attention(Q, K, V) \n",
    "print(output.shape) \n",
    "#print(output) \n",
    "print(Q)\n",
    "print(K[0])\n",
    "\n",
    "\n",
    "# torch.Size([1, 1, 32]) ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "torch.Size([12, 1])\n",
      "1\n",
      "tensor([[ 3.7421e-01,  4.7305e-02,  2.4503e-03, -1.7902e-03,  6.8952e-02,\n",
      "          1.5911e-01, -1.1148e-01,  3.2580e-01,  2.3623e-02,  1.6507e-01,\n",
      "          2.8500e-02,  1.7523e-02,  1.3803e-01,  2.0690e-01,  2.4201e-02,\n",
      "         -1.1153e-01,  8.0610e-03,  1.5245e-01, -7.2237e-02,  8.8497e-02,\n",
      "         -1.5001e-01, -1.4662e-01,  4.8354e-01, -6.8436e-02,  2.4036e-01,\n",
      "          1.2587e-01, -2.9741e-01, -1.9121e-02, -3.4182e-01, -3.6344e-02,\n",
      "         -1.1900e-01,  1.6724e-01,  3.6068e-01, -7.9976e-02,  1.6924e-01,\n",
      "         -5.1975e-02, -1.9620e-01, -2.6547e-03,  1.1183e-01, -3.5658e-02,\n",
      "          4.1525e-01,  2.4581e-01, -4.7811e-01,  1.6771e-01,  1.8765e-01,\n",
      "          3.5561e-02,  2.0565e-01,  1.7503e-01,  6.9023e-02, -2.1510e-01,\n",
      "         -3.2191e-01, -2.7941e-01,  1.9586e-01,  1.6022e-01,  9.0823e-02,\n",
      "         -1.2894e-01, -2.6922e-01,  9.0844e-02,  1.4083e-01, -2.0077e-01,\n",
      "          5.7030e-02,  8.1929e-02, -6.1835e-02,  1.0544e-01],\n",
      "        [ 3.7658e-01,  5.0024e-02, -2.2891e-01,  2.3543e-02,  1.0575e-01,\n",
      "          1.0621e-01, -1.9568e-01,  3.2739e-01,  6.2386e-02,  1.3057e-01,\n",
      "          8.9980e-02,  9.2943e-02,  3.0694e-01,  2.3826e-01,  1.1554e-01,\n",
      "         -1.7533e-01,  9.7901e-03,  1.2007e-01, -2.4041e-02,  5.6766e-02,\n",
      "         -1.0122e-01, -6.9482e-02,  4.2893e-01, -3.0429e-02,  3.3788e-01,\n",
      "          2.4443e-01, -3.6491e-01, -1.2102e-01, -4.2655e-01, -1.9290e-02,\n",
      "         -1.6379e-01,  1.8387e-01,  2.6023e-01, -2.3589e-02,  1.5253e-01,\n",
      "         -9.5313e-02, -1.2560e-01, -2.0970e-02,  2.0722e-01,  7.0344e-02,\n",
      "          5.4457e-01,  1.1706e-01, -5.4091e-01,  1.4341e-01,  9.3948e-02,\n",
      "          9.7617e-02,  2.8248e-01,  1.0999e-01,  6.0252e-02, -2.0951e-01,\n",
      "         -2.9871e-01, -2.4308e-01,  2.4444e-01,  1.7182e-01,  2.2603e-01,\n",
      "         -1.1472e-01, -2.9533e-01, -3.6363e-02,  6.1169e-02, -3.2665e-02,\n",
      "          1.3076e-02,  1.4831e-01, -8.8879e-02, -8.1423e-03],\n",
      "        [ 3.8552e-01,  2.6630e-02,  5.1600e-02, -5.2339e-03,  3.6689e-02,\n",
      "          9.1157e-02, -6.9972e-02,  2.9513e-01,  8.9332e-02,  1.5979e-01,\n",
      "          1.4258e-01, -6.9746e-02,  2.1201e-01,  2.3889e-01, -5.0016e-02,\n",
      "         -9.2905e-02,  8.5909e-03,  1.3947e-01, -3.2713e-03, -2.1828e-02,\n",
      "         -1.1178e-01, -1.3531e-01,  3.9545e-01, -6.9543e-02,  1.4967e-01,\n",
      "          1.3148e-01, -2.9673e-01,  3.7461e-02, -3.5599e-01, -1.2174e-01,\n",
      "         -1.0330e-01,  1.1663e-01,  3.7430e-01, -5.5210e-02,  2.0729e-01,\n",
      "          6.3373e-02, -8.8411e-02, -6.3225e-02,  2.6785e-03, -9.6480e-02,\n",
      "          4.4155e-01,  2.5311e-01, -4.5725e-01,  1.1552e-01,  1.7260e-01,\n",
      "         -1.1632e-02,  8.2563e-02,  9.2462e-02,  8.5152e-02, -2.4328e-01,\n",
      "         -2.8435e-01, -1.8790e-01,  1.5548e-01,  1.6295e-01,  3.9326e-02,\n",
      "         -2.2739e-01, -2.0555e-01,  1.2270e-01,  2.0045e-01, -1.3884e-01,\n",
      "          2.3926e-02,  2.0643e-01, -2.4054e-02,  1.2911e-01],\n",
      "        [ 4.2087e-01,  4.1271e-02, -3.7992e-02,  5.6348e-02,  5.1114e-02,\n",
      "          4.5123e-02, -7.2758e-02,  3.1932e-01,  5.2185e-02,  1.4013e-01,\n",
      "          6.6667e-02, -1.2149e-02,  2.5602e-01,  1.7746e-01, -4.3208e-02,\n",
      "         -1.3507e-01, -2.9570e-02,  2.1424e-01, -2.0669e-02,  8.5294e-03,\n",
      "         -7.6525e-02, -8.7690e-02,  4.5064e-01, -8.4053e-03,  2.4623e-01,\n",
      "          1.0714e-01, -3.4966e-01,  1.4199e-02, -2.9863e-01, -8.8328e-02,\n",
      "         -1.1914e-01,  2.2059e-01,  3.0435e-01, -1.7253e-01,  2.1237e-01,\n",
      "         -9.7731e-02, -1.1178e-01, -5.7360e-02,  9.8084e-02,  8.8640e-03,\n",
      "          4.6409e-01,  2.5563e-01, -4.7423e-01,  8.2827e-02,  1.7840e-01,\n",
      "          1.0132e-02,  1.3924e-01,  1.5260e-01,  8.0349e-02, -2.0995e-01,\n",
      "         -1.9666e-01, -2.0878e-01,  6.4266e-02,  2.0937e-01,  1.2846e-01,\n",
      "         -1.6567e-01, -2.8593e-01,  1.3638e-01,  9.2040e-02, -7.8893e-02,\n",
      "         -1.4246e-02,  1.7518e-01, -5.4842e-02,  7.1478e-02],\n",
      "        [ 4.1563e-01,  2.4535e-02, -2.1761e-01, -1.7748e-01,  5.1978e-02,\n",
      "          9.1245e-02, -1.0134e-01,  2.4934e-01,  1.0204e-01,  7.0894e-02,\n",
      "          2.1058e-01,  5.4574e-02,  2.2634e-01,  3.3350e-01,  9.6868e-02,\n",
      "         -7.1571e-02,  4.0513e-02,  9.4664e-02, -2.0533e-02,  5.9374e-02,\n",
      "         -4.0986e-02,  2.6153e-03,  1.4676e-01,  4.8304e-02,  2.5978e-01,\n",
      "          2.2430e-01, -3.5502e-01, -1.4355e-01, -5.1927e-01, -1.2312e-01,\n",
      "         -2.7164e-02,  7.8583e-02,  1.9687e-01,  5.3238e-02,  2.5315e-01,\n",
      "         -6.6858e-02,  5.8641e-02,  6.3495e-02,  1.7595e-01,  1.1578e-01,\n",
      "          4.4619e-01,  1.7339e-01, -5.0421e-01,  1.6427e-01,  1.6037e-01,\n",
      "          2.9845e-02,  2.6452e-01,  1.4162e-01,  1.8532e-01, -2.9652e-01,\n",
      "         -3.5264e-01, -1.6369e-01,  1.3530e-01,  1.2585e-01,  2.7547e-01,\n",
      "         -2.1311e-01, -1.5485e-01, -4.8980e-03,  1.1283e-01, -1.1869e-01,\n",
      "         -2.7321e-03,  1.6533e-01, -1.2598e-01, -1.3980e-01],\n",
      "        [ 3.8654e-01,  1.3049e-02,  3.4992e-02, -9.7621e-02,  9.5578e-02,\n",
      "          1.4366e-01, -8.9994e-02,  2.9733e-01,  6.1718e-02,  1.3184e-01,\n",
      "          1.9331e-01,  5.1499e-02,  7.8288e-02,  2.2208e-01,  1.9691e-02,\n",
      "         -1.6806e-01,  1.3185e-02,  1.7471e-01, -7.2262e-02,  3.3013e-02,\n",
      "         -1.2426e-01, -1.4770e-01,  4.2425e-01, -3.2459e-02,  2.3744e-01,\n",
      "          1.6093e-01, -2.6957e-01, -1.1826e-01, -3.4049e-01, -5.7039e-02,\n",
      "         -9.4733e-02,  1.2298e-01,  3.8689e-01,  3.5659e-02,  2.2087e-01,\n",
      "         -1.5148e-03, -1.7236e-01,  2.8596e-02,  6.6166e-02, -1.3043e-01,\n",
      "          4.7793e-01,  1.7187e-01, -4.6038e-01,  1.9134e-01,  2.7852e-01,\n",
      "          6.1530e-02,  1.8298e-01,  1.4375e-01,  6.2690e-02, -1.6792e-01,\n",
      "         -3.0137e-01, -2.7293e-01,  2.5169e-01,  8.9886e-02,  2.1522e-02,\n",
      "         -2.2829e-01, -2.1659e-01,  4.0652e-02,  2.0073e-01, -2.1858e-01,\n",
      "          3.9766e-02,  1.5262e-01, -5.5053e-02,  6.9060e-02],\n",
      "        [ 3.8924e-01,  5.8152e-02, -1.8165e-02,  2.3887e-02, -2.4625e-02,\n",
      "          9.7413e-02, -7.6765e-02,  2.9088e-01,  8.4818e-02,  1.5713e-01,\n",
      "         -2.6939e-02, -5.1142e-02,  2.3761e-01,  2.2048e-01,  6.9481e-02,\n",
      "         -1.8503e-02, -2.4933e-02,  6.5321e-02, -5.7195e-02,  6.5645e-02,\n",
      "         -6.9247e-02, -6.4582e-02,  3.5944e-01, -2.7634e-02,  1.6530e-01,\n",
      "          1.1485e-01, -3.4943e-01,  9.8451e-02, -4.3325e-01, -3.6218e-02,\n",
      "         -6.3220e-02,  1.3738e-01,  2.6545e-01, -2.1009e-01,  1.5777e-01,\n",
      "         -9.1504e-02, -5.9366e-02, -4.8823e-02,  1.1661e-01,  8.4317e-02,\n",
      "          3.1847e-01,  3.4974e-01, -5.1344e-01,  1.2761e-01,  1.3596e-01,\n",
      "          2.0157e-02,  1.9648e-01,  2.0604e-01,  1.5587e-01, -3.0771e-01,\n",
      "         -3.8969e-01, -1.8519e-01,  3.2242e-02,  2.4064e-01,  2.2743e-01,\n",
      "         -1.1479e-01, -2.5088e-01,  1.3776e-01,  1.0788e-01, -1.1551e-01,\n",
      "          5.6492e-02,  1.3963e-01, -1.0901e-01,  5.9104e-02],\n",
      "        [ 4.2004e-01,  5.7933e-02, -3.7215e-02,  5.0573e-02, -7.7596e-03,\n",
      "          7.6061e-02, -6.9643e-02,  3.3045e-01,  2.2497e-02,  1.7737e-01,\n",
      "         -1.8081e-02, -1.1217e-01,  2.7980e-01,  2.1361e-01, -6.3502e-02,\n",
      "         -4.0337e-02, -1.6430e-02,  1.9206e-01,  6.7838e-03,  5.2858e-02,\n",
      "         -8.7818e-02, -1.1395e-01,  3.7831e-01, -7.9812e-02,  2.5128e-01,\n",
      "          7.4336e-02, -3.4281e-01,  1.1365e-01, -4.0340e-01, -1.9199e-01,\n",
      "         -1.1316e-01,  1.4389e-01,  3.1071e-01, -1.5032e-01,  1.8157e-01,\n",
      "         -6.0128e-02, -8.5805e-02, -8.4022e-02,  8.3662e-02,  3.4423e-02,\n",
      "          3.7129e-01,  2.9098e-01, -4.6800e-01,  7.9425e-02,  1.2983e-01,\n",
      "         -1.0195e-01,  1.5491e-01,  1.6780e-01,  1.3190e-01, -3.1523e-01,\n",
      "         -2.2061e-01, -1.8750e-01, -1.6820e-02,  2.5642e-01,  1.5092e-01,\n",
      "         -1.2669e-01, -2.8396e-01,  1.6979e-01,  9.4507e-02, -8.5562e-02,\n",
      "          2.5036e-02,  1.2992e-01, -1.1157e-02,  7.8776e-02],\n",
      "        [ 3.9836e-01,  4.3075e-02, -4.1099e-02, -8.7179e-03,  3.3797e-02,\n",
      "          8.6127e-02, -9.0071e-02,  2.6499e-01,  7.4645e-02,  1.4069e-01,\n",
      "          9.3599e-02, -6.0060e-02,  2.6440e-01,  2.6281e-01, -2.2011e-02,\n",
      "         -6.6702e-02,  3.8912e-02,  1.3570e-01,  2.0280e-02,  2.8238e-02,\n",
      "         -1.0808e-01, -4.8517e-02,  3.4670e-01, -9.0889e-02,  1.8911e-01,\n",
      "          1.2322e-01, -3.4025e-01,  3.1523e-02, -3.6881e-01, -8.7365e-02,\n",
      "         -1.1840e-01,  1.8014e-01,  3.4659e-01, -1.0805e-01,  1.8847e-01,\n",
      "         -2.4956e-02, -6.6929e-02, -3.9569e-02,  4.1022e-02,  2.0428e-02,\n",
      "          4.1195e-01,  3.1657e-01, -5.0841e-01,  8.8468e-02,  1.0037e-01,\n",
      "         -1.4397e-02,  1.0291e-01,  1.0779e-01,  8.3237e-02, -3.1607e-01,\n",
      "         -3.4460e-01, -1.9386e-01,  1.4447e-01,  1.9536e-01,  1.5387e-01,\n",
      "         -1.7341e-01, -1.9851e-01,  1.4411e-01,  1.7500e-01, -1.2065e-01,\n",
      "          1.9488e-02,  1.2949e-01, -1.4518e-02,  7.1196e-02],\n",
      "        [ 3.8802e-01,  6.5347e-02, -4.8477e-02,  3.9209e-02,  1.1707e-02,\n",
      "          1.1652e-01, -9.4445e-02,  3.3135e-01,  5.4462e-02,  1.5712e-01,\n",
      "          1.9739e-02, -2.8490e-02,  2.3276e-01,  2.1753e-01,  2.0018e-02,\n",
      "         -6.5892e-02, -2.4871e-02,  1.2139e-01, -6.6972e-02,  4.4623e-02,\n",
      "         -1.1013e-01, -1.4178e-01,  4.3875e-01, -4.3854e-03,  1.8791e-01,\n",
      "          1.3364e-01, -3.4690e-01,  5.8643e-02, -3.7844e-01, -9.4994e-02,\n",
      "         -1.1193e-01,  1.4358e-01,  2.6328e-01, -1.7923e-01,  1.7972e-01,\n",
      "         -2.5581e-02, -7.9780e-02, -6.6555e-02,  1.3033e-01,  2.0945e-02,\n",
      "          4.1753e-01,  2.3488e-01, -4.7476e-01,  1.2210e-01,  1.4268e-01,\n",
      "          2.3747e-02,  2.0510e-01,  1.6392e-01,  1.1697e-01, -2.3003e-01,\n",
      "         -2.4889e-01, -1.6858e-01,  7.0222e-02,  2.3696e-01,  1.5111e-01,\n",
      "         -1.3516e-01, -2.9091e-01,  8.5944e-02,  6.3975e-02, -9.1527e-02,\n",
      "          4.4352e-03,  1.6729e-01, -8.5715e-02,  7.4373e-02],\n",
      "        [ 3.8768e-01,  6.8489e-02, -3.7118e-03,  6.1017e-02,  9.9582e-02,\n",
      "          1.5154e-01, -1.0816e-01,  3.9006e-01, -2.6191e-02,  1.8885e-01,\n",
      "         -1.3519e-02,  2.6530e-02,  1.3398e-01,  1.9254e-01, -1.3290e-02,\n",
      "         -1.7087e-01,  1.0143e-02,  1.9724e-01, -1.0903e-01,  1.0040e-01,\n",
      "         -1.6500e-01, -2.1370e-01,  5.4897e-01, -7.7183e-02,  2.8246e-01,\n",
      "          8.3824e-02, -2.9321e-01,  1.5889e-02, -3.0068e-01, -5.6291e-02,\n",
      "         -1.2856e-01,  1.7381e-01,  3.9204e-01, -7.3298e-02,  1.4522e-01,\n",
      "         -2.0929e-02, -2.2489e-01, -2.6572e-02,  1.3111e-01, -7.3562e-02,\n",
      "          4.0709e-01,  1.8809e-01, -4.3260e-01,  1.7288e-01,  2.1834e-01,\n",
      "         -6.0111e-03,  2.1058e-01,  1.9949e-01,  6.3944e-02, -1.7139e-01,\n",
      "         -2.3643e-01, -2.8592e-01,  1.7339e-01,  1.8411e-01,  3.6993e-02,\n",
      "         -9.8956e-02, -3.1819e-01,  9.6136e-02,  9.2699e-02, -2.0339e-01,\n",
      "          5.0560e-02,  9.3483e-02, -4.0553e-02,  1.6463e-01],\n",
      "        [ 3.9933e-01,  4.7766e-02, -1.4683e-01, -2.0178e-02,  8.7706e-02,\n",
      "          1.2712e-01, -1.2991e-01,  3.6296e-01,  2.7058e-02,  1.4473e-01,\n",
      "          5.5421e-02,  6.8513e-02,  1.9678e-01,  2.3651e-01,  8.2345e-02,\n",
      "         -1.5040e-01, -6.7540e-03,  1.3477e-01, -8.6206e-02,  9.2523e-02,\n",
      "         -9.4123e-02, -1.3373e-01,  3.9667e-01, -3.5472e-03,  3.1254e-01,\n",
      "          1.8311e-01, -3.2425e-01, -7.7701e-02, -4.4103e-01, -7.8183e-02,\n",
      "         -8.4265e-02,  1.0501e-01,  2.7167e-01,  6.9543e-03,  1.7145e-01,\n",
      "         -5.9190e-02, -1.1698e-01,  3.6032e-03,  2.0628e-01,  2.9678e-02,\n",
      "          4.4422e-01,  1.1886e-01, -4.7306e-01,  1.9200e-01,  1.9716e-01,\n",
      "          3.0745e-02,  3.0549e-01,  1.9381e-01,  1.3302e-01, -2.0005e-01,\n",
      "         -2.7550e-01, -2.4281e-01,  1.5169e-01,  1.6439e-01,  1.7148e-01,\n",
      "         -1.2008e-01, -2.9223e-01,  4.6167e-05,  5.9890e-02, -1.2432e-01,\n",
      "          3.8789e-02,  1.4654e-01, -1.1794e-01,  1.2930e-02]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.query = nn.Linear(in_dim, hidden_dim) #查询\n",
    "        self.key = nn.Linear(in_dim, hidden_dim)\n",
    "        self.value = nn.Linear(in_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        公式为：Attention(Q, K, V) = softmax(Q*K^T/sqrt(d_k)) * V\n",
    "        Q: 查询\n",
    "        K: 键\n",
    "        V: 值\n",
    "        d_k: 键的维度 这里用hidden_dim表示 即:K.size(-1)\n",
    "        对张量 K 进行转置（transpose）。具体来说，这个操作会将张量 K 的第0维和第1维进行交换\n",
    "        '''\n",
    "        Q = self.query(x) \n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(torch.tensor(K.size(-1), dtype=torch.float32))\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attended_values = torch.matmul(attn_probs, V)\n",
    "        return attended_values\n",
    "\n",
    "class MLPNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim_1=256, hidden_dim_2=128, attention_dim=64):\n",
    "        super(MLPNetworkWithAttention, self).__init__()\n",
    "        self.attention = Attention(in_dim, attention_dim)\n",
    "        self.fc1 = torch.nn.Linear(attention_dim, hidden_dim_1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim_2, out_dim)\n",
    "        \n",
    "        # 根据计算增益\n",
    "        gain1 = nn.init.calculate_gain('relu')\n",
    "        # Xavier均匀分布初始化\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight, gain=gain1)\n",
    "        # 初始化参数\n",
    "        self.fc1.bias.data.fill_(0.01)\n",
    "        self.fc2.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, x): #这里的x是状态加动作\n",
    "        x = self.attention(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 创建带有注意力机制的critic\n",
    "global_obs_dim = 128  # 示例输入维度\n",
    "critic = MLPNetworkWithAttention(global_obs_dim, 1)\n",
    "#print(critic)\n",
    "query = nn.Linear(128, 1)\n",
    "x = torch.randn(12, 128)\n",
    "Q = query(x)\n",
    "print(Q.transpose(0, 1).size())\n",
    "print(Q.size()) #.size表示维度\n",
    "print(Q.size(-1))\n",
    "an = Attention(128, 64)\n",
    "output = an(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiAgentAttention(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(MultiAgentAttention, self).__init__()\n",
    "        self.query = nn.Linear(in_dim, hidden_dim)\n",
    "        self.key = nn.Linear(in_dim, hidden_dim)\n",
    "        self.value = nn.Linear(in_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, agent_states):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            agent_states (list of tensors): List of states for each agent\n",
    "        Outputs:\n",
    "            attended_values (tensor): Attention-weighted values for each agent\n",
    "        \"\"\"\n",
    "        Q = [self.query(state) for state in agent_states]\n",
    "        K = [self.key(state) for state in agent_states]\n",
    "        V = [self.value(state) for state in agent_states]\n",
    "        \n",
    "        Q = torch.stack(Q)\n",
    "        K = torch.stack(K)\n",
    "        V = torch.stack(V)\n",
    "        \n",
    "        attn_scores = torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(torch.tensor(K.size(-1), dtype=torch.float32))\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attended_values = torch.matmul(attn_probs, V)\n",
    "        \n",
    "        return attended_values\n",
    "\n",
    "class MultiAgentMLPNetworkWithAttention(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim_1=256, hidden_dim_2=128, attention_dim=64, ):\n",
    "        super(MultiAgentMLPNetworkWithAttention, self).__init__()\n",
    "        self.attention = MultiAgentAttention(in_dim, attention_dim)\n",
    "        self.fc1 = torch.nn.Linear(attention_dim, hidden_dim_1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim_2, out_dim)\n",
    "        \n",
    "        # 根据计算增益\n",
    "        gain1 = nn.init.calculate_gain('relu')\n",
    "        # Xavier均匀分布初始化\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight, gain=gain1)\n",
    "        # 初始化参数\n",
    "        self.fc1.bias.data.fill_(0.01)\n",
    "        self.fc2.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, agent_states):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            agent_states (list of tensors): List of states for each agent\n",
    "        Outputs:\n",
    "            outputs (list of tensors): Outputs for each agent\n",
    "        \"\"\"\n",
    "        attended_values = self.attention(agent_states)\n",
    "        outputs = []\n",
    "        for value in attended_values:\n",
    "            x = F.relu(self.fc1(value))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            output = self.fc3(x)\n",
    "            outputs.append(output)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  7,  8,  9, 13, 14, 17, 18],\n",
      "        [ 4,  5,  6, 10, 11, 12, 15, 16, 19, 20]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设 state_list 包含两个张量，每个张量的形状为 (batch_size, state_dim)\n",
    "state_list = [\n",
    "    torch.tensor([[1, 2, 3], [4, 5, 6]]),  # 形状为 (2, 3)\n",
    "    torch.tensor([[7, 8, 9], [10, 11, 12]])  # 形状为 (2, 3)\n",
    "]\n",
    "\n",
    "# 假设 act_list 包含两个张量，每个张量的形状为 (batch_size, action_dim)\n",
    "act_list = [\n",
    "    torch.tensor([[13, 14], [15, 16]]),  # 形状为 (2, 2)\n",
    "    torch.tensor([[17, 18], [19, 20]])  # 形状为 (2, 2)\n",
    "]\n",
    "x = torch.cat(state_list + act_list, 1) #按列拼接\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "que torch.Size([2, 4]) 2 4\n",
      "key torch.Size([2, 5, 4])\n",
      "val torch.Size([2, 5, 4])\n",
      "torch.Size([2, 1, 4])\n",
      "torch.Size([5, 4, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m e_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# 其余agent状态编码后的e_k # 2个样本，5个智能体，3个维度\u001b[39;00m\n\u001b[0;32m     66\u001b[0m attention \u001b[38;5;241m=\u001b[39m Attention_block(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# 3是输入维度，4是隐藏层维度\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43me_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_k\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\softwares\\anaconda3\\envs\\myrl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\softwares\\anaconda3\\envs\\myrl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 39\u001b[0m, in \u001b[0;36mAttention_block.forward\u001b[1;34m(self, e_q, e_k)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(keys\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 矩阵相乘，3维的。第一维不变（batchsize），第二维和第三维是要相乘的矩阵。\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# view是把维度变化下，（batchsize）*1*（context_size）\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m weight_log \u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03mweight_log = torch.bmm(query.view(query.shape[0], 1, self.context_size),keys.view(keys.shape[0], self.context_size, -1).T)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(weight_log\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_dim):\n",
    "        super(Attention_block,self).__init__()\n",
    "\n",
    "        self.key_extractors=nn.Linear(embedding_size, hidden_dim, bias=False)\n",
    "        self.selector_extractors=nn.Linear(embedding_size, hidden_dim, bias=False)\n",
    "        self.value_extractors=nn.Sequential(nn.Linear(embedding_size,hidden_dim),nn.LeakyReLU())\n",
    "\n",
    "        self.context_size = hidden_dim\n",
    "    '''\n",
    "        self.W_k = nn.Linear(embedding_size, hidden_dim)  # 所有智能体共享\n",
    "\n",
    "        self.W_q = nn.Linear(embedding_size, hidden_dim)\n",
    "\n",
    "        self.V = nn.Linear(embedding_size, hidden_dim)  # 转换为信息向量（关键信息）\n",
    "\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.context_size = hidden_dim\n",
    "    '''\n",
    "    #input: 当前agent状态编码后E_q,其余agent状态编码后的e_k(e_k 总共5个智能体)\n",
    "    # output：其余agent状态对当前agent决策贡献的信息，是一个向量xi\n",
    "    def forward(self, e_q, e_k):\n",
    "        query = self.selector_extractors(e_q)  # 转换为索引\n",
    "        print('que',query.shape,query.shape[0],query.shape[1])\n",
    "        keys = self.key_extractors(e_k)  # 转换为关键字，为了比较重要性\n",
    "        values=self.value_extractors(e_k)\n",
    "        print('key',keys.shape)\n",
    "        print('val',values.shape)\n",
    "        #print(query.view(query.shape[0], 1, self.context_size).shape) #.view是维度变化 本身不变\n",
    "        #print(query.shape)\n",
    "        print(query.view(query.shape[0], 1, -1).shape)\n",
    "        #print(keys.view(keys.shape[1], self.context_size, -1).shape)\n",
    "        print(keys.permute(0, 2, 1).shape)\n",
    "        # 矩阵相乘，3维的。第一维不变（batchsize），第二维和第三维是要相乘的矩阵。\n",
    "        # view是把维度变化下，（batchsize）*1*（context_size）\n",
    "        weight_log =torch.matmul(query.view(query.shape[0], 1, -1),keys.permute(0, 2, 1))\n",
    "        '''\n",
    "        weight_log = torch.bmm(query.view(query.shape[0], 1, self.context_size),keys.view(keys.shape[0], self.context_size, -1).T)\n",
    "        '''\n",
    "        print(weight_log.shape)\n",
    "        scaled_weight_log=weight_log/np.sqrt(keys[0].shape[1])\n",
    "        print(scaled_weight_log.shape)\n",
    "        print(keys[0].shape[1])\n",
    "        attend_weights = torch.softmax(scaled_weight_log, dim=2) # 对第二维度归一化，0到1，每一个的重要程度 2*1*5\n",
    "        print(attend_weights.shape)\n",
    "        print(values.permute(0, 2, 1).shape)\n",
    "        attention_values = (values.permute(0, 2, 1) * attend_weights).sum(dim=2) # 2*4*5 * 2*1*5 -> 2*4*5\n",
    "        print(attention_values.shape)\n",
    "        '''\n",
    "        weight = torch.softmax(scaled_weight_log, dim=2)  # batchsize*1*5的矩阵，对第二维度归一化，0到1，每一个的重要程度\n",
    "        \n",
    "        # print(weight.shape)\n",
    "        # print(self.ReLU(self.V(e_k)).shape)\n",
    "\n",
    "        # 每一个状态对我当前状态的重要性，做一个加权和\n",
    "        attention_context = torch.bmm(weight, self.ReLU(self.V(e_k)).view(query.shape[0], -1, self.context_size))\n",
    "        '''\n",
    "        return attention_values   # batchsize*1*context_size\n",
    "    \n",
    "# 例子\n",
    "e_q = torch.randn(2, 3)  # 当前agent状态编码后E_q # 2个样本，3个维度\n",
    "e_k = torch.randn(2, 5, 3)  # 其余agent状态编码后的e_k # 2个样本，5个智能体，3个维度\n",
    "attention = Attention_block(3, 4) # 3是输入维度，4是隐藏层维度\n",
    "output = attention(e_q, e_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.3392],\n",
      "        [-0.3593]], grad_fn=<AddmmBackward0>), tensor([[-0.1796],\n",
      "        [-0.1811]], grad_fn=<AddmmBackward0>)]\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "\n",
    "torch.manual_seed(1)\n",
    "class CriticBase(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CriticBase, self).__init__()\n",
    "        self.args = args\n",
    "        self._define_parameters()\n",
    "\n",
    "    def _define_parameters_for_hidden_layers(self, parameters_dict, agent_index=None):\n",
    "        pass\n",
    "\n",
    "    def _define_parameters(self):\n",
    "        self.parameters_all_agent = nn.ModuleList()  # do not use python list []\n",
    "        for i in range(self.args.agent_count):\n",
    "            parameters_dict = nn.ModuleDict()  # do not use python dict {}\n",
    "            # parameters for pre-processing observations and actions\n",
    "            parameters_dict[\"fc_obs\"] = nn.Linear(self.args.observation_dim_list[i], self.args.hidden_dim)\n",
    "            parameters_dict[\"fc_action\"] = nn.Linear(self.args.action_dim_list[i], self.args.hidden_dim)\n",
    "\n",
    "            # parameters for hidden layers\n",
    "            self._define_parameters_for_hidden_layers(parameters_dict, i)\n",
    "\n",
    "            # parameters for generating Qvalues\n",
    "            parameters_dict[\"Qvalue\"] = nn.Linear(self.args.hidden_dim, 1)\n",
    "            self.parameters_all_agent.append(parameters_dict)\n",
    "\n",
    "    def _forward_of_hidden_layers(self, out_obs_list, out_action_list):\n",
    "        pass\n",
    "\n",
    "    def forward(self, observation_batch_list, action_batch_list):\n",
    "        # pre-process\n",
    "        out_obs_list, out_action_list = [], []\n",
    "        for i in range(self.args.agent_count):\n",
    "            out_obs = F.relu(self.parameters_all_agent[i][\"fc_obs\"](observation_batch_list[i]))\n",
    "            out_action = F.relu(self.parameters_all_agent[i][\"fc_action\"](action_batch_list[i]))\n",
    "            out_obs_list.append(out_obs)\n",
    "            out_action_list.append(out_action)\n",
    "\n",
    "        # key part of difference MARL methods #\n",
    "        out_hidden_list = self._forward_of_hidden_layers(out_obs_list, out_action_list)\n",
    "        # if self.args.agent_name == \"NCC_AC\":\n",
    "        #     out_hidden_list, C_hat_list, obs_hat_list, action_hat_list = out_hidden_list\n",
    "        # elif self.args.agent_name == \"Contrastive\":\n",
    "        #     out_hidden_list, C_hat_list = out_hidden_list\n",
    "\n",
    "        # post-process\n",
    "        Qvalue_list = []\n",
    "        for i in range(self.args.agent_count):\n",
    "            Qvalue = self.parameters_all_agent[i][\"Qvalue\"](out_hidden_list[i])  # linear activation for Q-value\n",
    "            Qvalue_list.append(Qvalue)\n",
    "\n",
    "        # if self.args.agent_name == \"NCC_AC\":\n",
    "        #     return (Qvalue_list, C_hat_list, obs_hat_list, action_hat_list)\n",
    "        # elif self.args.agent_name == \"Contrastive\":\n",
    "        #     return (Qvalue_list, C_hat_list)\n",
    "        # else:\n",
    "        #     return Qvalue_list\n",
    "        return Qvalue_list\n",
    "        \n",
    "\n",
    "class CriticAttentionalMADDPG(CriticBase):\n",
    "    def __init__(self, args):\n",
    "        super(CriticAttentionalMADDPG, self).__init__(args)\n",
    "\n",
    "    def _define_parameters_for_hidden_layers(self, parameters_dict, agent_index=None):\n",
    "        hidden_dim = self.args.hidden_dim\n",
    "        head_dim = hidden_dim\n",
    "        encoder_input_dim = hidden_dim * (self.args.agent_count + 1) # 1 is for the action of the current agent\n",
    "        decoder_input_dim = hidden_dim * (self.args.agent_count - 1) #\n",
    "\n",
    "        parameters_dict[\"fc_encoder_input\"] = nn.Linear(encoder_input_dim, hidden_dim)\n",
    "        for k in range(self.args.head_count):\n",
    "            parameters_dict[\"fc_encoder_head\" + str(k)] = nn.Linear(hidden_dim, head_dim)\n",
    "\n",
    "        parameters_dict[\"fc_decoder_input\"] = nn.Linear(decoder_input_dim, head_dim)\n",
    "\n",
    "    def _global_attention(self, encoder_H, decoder_H):\n",
    "        # encoder_H 用作键值对，decoder_H 用作查询 也是当前目标向量\n",
    "        # encoder_H has a shape (source_vector_count, batch_size, hidden_dim)\n",
    "        # decoder_H has a shape (batch_size, hidden_dim)\n",
    "        # scores is based on \"dot-product\" function, it works well for the global attention #zh-cn: 基于“点积”函数的分数，对于全局注意力效果很好\n",
    "        temp_scores = torch.mul(encoder_H, decoder_H)  # (source_vector_count, batch_size, hidden_dim)\n",
    "        scores = torch.sum(temp_scores, dim=2)  # (source_vector_count, batch_size)\n",
    "        attention_weights = F.softmax(scores.permute(1, 0), dim=1)  # (batch_size, source_vector_count)\n",
    "        attention_weights = torch.unsqueeze(attention_weights, dim=2)  # (batch_size, source_vector_count, 1)\n",
    "        contextual_vector = torch.matmul(encoder_H.permute(1, 2, 0), attention_weights)  # (batch_size, hidden_dim, 1)\n",
    "        contextual_vector = torch.squeeze(contextual_vector)  # (batch_size, hidden_dim)\n",
    "        return contextual_vector\n",
    "\n",
    "    # in fact, K-head module and attention module are integrated into one module\n",
    "    def _attention_module(self, obs_list, action_list, agent_index):\n",
    "        encoder_input_list = obs_list + [action_list[agent_index]] # batch_size * (agent_count + 1) * obs_dim\n",
    "        decoder_input_list = action_list[:agent_index] + action_list[agent_index + 1:]\n",
    "\n",
    "        # generating a temp hidden layer \"h\" (the encoder part, refer the figure in our paper)\n",
    "        encoder_input = torch.cat(encoder_input_list, dim=1) \n",
    "        encoder_h = F.relu(self.parameters_all_agent[agent_index][\"fc_encoder_input\"](encoder_input))\n",
    "\n",
    "        # generating action-conditional Q-value heads (i.e., the encoder part)\n",
    "        encoder_head_list = []\n",
    "        for k in range(self.args.head_count):\n",
    "            encoder_head = F.relu(self.parameters_all_agent[agent_index][\"fc_encoder_head\" + str(k)](encoder_h))\n",
    "            encoder_head_list.append(encoder_head)\n",
    "        encoder_heads = torch.stack(encoder_head_list, dim=0)  # (head_count, batch_size, head_dim)\n",
    "\n",
    "        # generating a temp hidden layer \"H\" (the decoder part, refer the figure in our paper)\n",
    "        decoder_input = torch.cat(decoder_input_list, dim=1)\n",
    "        decoder_H = F.relu(self.parameters_all_agent[agent_index][\"fc_decoder_input\"](decoder_input))\n",
    "\n",
    "        # generating content vector (i.e., the decoder part)\n",
    "        contextual_vector = self._global_attention(encoder_heads, decoder_H)  # (batch_size, head_dim)   ###!!!\n",
    "\n",
    "        # contextual_vector need to be further transformed into 1-dimension Q-value\n",
    "        # this will be done by the forward() function in CriticBase()\n",
    "\n",
    "        return contextual_vector\n",
    "\n",
    "    def _forward_of_hidden_layers(self, out_obs_list, out_action_list):\n",
    "        out_hidden_list = []\n",
    "        for i in range(self.args.agent_count):\n",
    "            out = self._attention_module(out_obs_list, out_action_list, i)\n",
    "            out_hidden_list.append(out)\n",
    "        return out_hidden_list\n",
    "    \n",
    "# 例子\n",
    "# 将字典转换为对象\n",
    "args_dict = {'agent_count': 2, 'observation_dim_list': [3, 3], 'hidden_dim': 4, 'action_dim_list': [2, 2], 'head_count': 3, 'agent_name': 'qita'}\n",
    "args = argparse.Namespace(**args_dict)\n",
    "critic = CriticAttentionalMADDPG(args)\n",
    "observation_batch_list = [torch.randn(2, 3), torch.randn(2, 3)]  # 2个智能体，每个智能体的状态维度为3 形状: 2(智能体个数) * batch_size(2) * 3\n",
    "action_batch_list = [torch.randn(2, 2), torch.randn(2, 2)]  # 2个智能体，每个智能体的动作维度为2 形状: 2(智能体个数) * batch_size(2) * 2\n",
    "output = critic(observation_batch_list, action_batch_list) \n",
    "print(output)\n",
    "print(output[0].shape)  # torch.Size([2, 1]) batch_size * 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.1795],\n",
      "        [0.1644]], grad_fn=<AddmmBackward0>), tensor([[-0.1597],\n",
      "        [-0.1720]], grad_fn=<AddmmBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, encoder_input_dim, decoder_input_dim, hidden_dim, head_count):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.fc_encoder_input = nn.Linear(encoder_input_dim, hidden_dim)\n",
    "        self.fc_encoder_heads = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(head_count)])\n",
    "        self.fc_decoder_input = nn.Linear(decoder_input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        # encoder_input shape: (batch_size, input_dim)\n",
    "        encoder_h = F.relu(self.fc_encoder_input(encoder_input))\n",
    "        # encoder_h shape: (batch_size, hidden_dim)\n",
    "\n",
    "        encoder_heads = torch.stack([F.relu(head(encoder_h)) for head in self.fc_encoder_heads], dim=0)\n",
    "        # encoder_heads shape: (head_count, batch_size, hidden_dim)\n",
    "\n",
    "        # decoder_input shape: (batch_size, input_dim)\n",
    "        decoder_H = F.relu(self.fc_decoder_input(decoder_input))\n",
    "        # decoder_H shape: (batch_size, hidden_dim)\n",
    "\n",
    "        scores = torch.sum(torch.mul(encoder_heads, decoder_H), dim=2)\n",
    "        # scores shape: (head_count, batch_size)\n",
    "\n",
    "        attention_weights = F.softmax(scores.permute(1, 0), dim=1).unsqueeze(2)\n",
    "        # attention_weights shape: (batch_size, head_count, 1)\n",
    "\n",
    "        contextual_vector = torch.matmul(encoder_heads.permute(1, 2, 0), attention_weights).squeeze()\n",
    "        # contextual_vector shape: (batch_size, hidden_dim)\n",
    "\n",
    "        return contextual_vector\n",
    "\n",
    "class CriticAttentionalMADDPG(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CriticAttentionalMADDPG, self).__init__()\n",
    "        self.args = args\n",
    "        self.fc_obs = nn.ModuleList([nn.Linear(obs_dim, args.hidden_dim) for obs_dim in args.observation_dim_list])\n",
    "        self.fc_action = nn.ModuleList([nn.Linear(action_dim, args.hidden_dim) for action_dim in args.action_dim_list])\n",
    "        self.attention_modules = nn.ModuleList([AttentionModule(args.hidden_dim * (args.agent_count + 1), args.hidden_dim * (args.agent_count - 1),args.hidden_dim, args.head_count) for _ in range(args.agent_count)])\n",
    "        self.fc_qvalue = nn.ModuleList([nn.Linear(args.hidden_dim, 1) for _ in range(args.agent_count)])\n",
    "\n",
    "    def forward(self, observation_batch_list, action_batch_list):\n",
    "        out_obs_list = [F.relu(fc_obs(obs)) for fc_obs, obs in zip(self.fc_obs, observation_batch_list)]\n",
    "        # out_obs_list shape: [(batch_size, hidden_dim), ...] #即 batch_size * hidden_dim * agent_count\n",
    "\n",
    "        out_action_list = [F.relu(fc_action(action)) for fc_action, action in zip(self.fc_action, action_batch_list)]\n",
    "        # out_action_list shape: [(batch_size, hidden_dim), ...]\n",
    "\n",
    "        qvalue_list = []\n",
    "        for i in range(self.args.agent_count):\n",
    "            encoder_input = torch.cat(out_obs_list + [out_action_list[i]], dim=1)\n",
    "            # encoder_input shape: (batch_size, hidden_dim * (agent_count + 1))\n",
    "\n",
    "            decoder_input = torch.cat(out_action_list[:i] + out_action_list[i+1:], dim=1)\n",
    "            # decoder_input shape: (batch_size, hidden_dim * (agent_count - 1))\n",
    "\n",
    "            contextual_vector = self.attention_modules[i](encoder_input, decoder_input)\n",
    "            # contextual_vector shape: (batch_size, hidden_dim)\n",
    "\n",
    "            qvalue = self.fc_qvalue[i](contextual_vector)\n",
    "            # qvalue shape: (batch_size, 1)\n",
    "\n",
    "            qvalue_list.append(qvalue)\n",
    "\n",
    "        return qvalue_list\n",
    "\n",
    "# 例子\n",
    "args_dict = {'agent_count': 2, 'observation_dim_list': [3, 3], 'hidden_dim': 4, 'action_dim_list': [2, 2], 'head_count': 3, 'agent_name': 'qita'}\n",
    "args = argparse.Namespace(**args_dict)\n",
    "critic = CriticAttentionalMADDPG(args)\n",
    "observation_batch_list = [torch.randn(2, 3), torch.randn(2, 3)]\n",
    "action_batch_list = [torch.randn(2, 2), torch.randn(2, 2)]\n",
    "output = critic(observation_batch_list, action_batch_list)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 改成 只输出当前agent的Q值\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, encoder_input_dim, decoder_input_dim, hidden_dim, head_count):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.fc_encoder_input = nn.Linear(encoder_input_dim, hidden_dim)\n",
    "        self.fc_encoder_heads = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(head_count)])\n",
    "        self.fc_decoder_input = nn.Linear(decoder_input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        # encoder_input shape: (batch_size, input_dim)\n",
    "        encoder_h = F.relu(self.fc_encoder_input(encoder_input))\n",
    "        # encoder_h shape: (batch_size, hidden_dim)\n",
    "\n",
    "        encoder_heads = torch.stack([F.relu(head(encoder_h)) for head in self.fc_encoder_heads], dim=0)\n",
    "        # encoder_heads shape: (head_count, batch_size, hidden_dim)\n",
    "\n",
    "        # decoder_input shape: (batch_size, input_dim)\n",
    "        decoder_H = F.relu(self.fc_decoder_input(decoder_input))\n",
    "        # decoder_H shape: (batch_size, hidden_dim)\n",
    "\n",
    "        scores = torch.sum(torch.mul(encoder_heads, decoder_H), dim=2)\n",
    "        # scores shape: (head_count, batch_size)\n",
    "\n",
    "        attention_weights = F.softmax(scores.permute(1, 0), dim=1).unsqueeze(2)\n",
    "        # attention_weights shape: (batch_size, head_count, 1)\n",
    "\n",
    "        contextual_vector = torch.matmul(encoder_heads.permute(1, 2, 0), attention_weights).squeeze()\n",
    "        # contextual_vector shape: (batch_size, hidden_dim)\n",
    "\n",
    "        return contextual_vector\n",
    "\n",
    "class CriticAttentionalMADDPG(nn.Module):\n",
    "    def __init__(self, hidden_dim,head_count):\n",
    "        super(CriticAttentionalMADDPG, self).__init__()\n",
    "        #self.args = args # 3为智能体个数 12为状态维度 1为动作维度 \n",
    "        self.fc_obs = nn.Linear(12, hidden_dim) #nn.ModuleList([nn.Linear(obs_dim, args.hidden_dim) for obs_dim in args.observation_dim_list])\n",
    "        self.fc_action = nn.Linear(1, hidden_dim)#nn.ModuleList([nn.Linear(action_dim, args.hidden_dim) for action_dim in args.action_dim_list])\n",
    "        self.attention_modules = AttentionModule(hidden_dim * (3 + 1), hidden_dim * (3 - 1),hidden_dim, head_count) #nn.ModuleList([AttentionModule(args.hidden_dim * (args.agent_count + 1), args.hidden_dim * (args.agent_count - 1),args.hidden_dim, args.head_count) for _ in range(args.agent_count)])\n",
    "        self.fc_qvalue = nn.Linear(hidden_dim, 1) #nn.ModuleList([nn.Linear(args.hidden_dim, 1) for _ in range(args.agent_count)])\n",
    "\n",
    "    def forward(self, x,agent_id,agents):\n",
    "        agent_id_list = list(agents.keys())\n",
    "        agent_id_index = agent_id_list.index(agent_id) #获取agent_id在agents中的索引 按照顺序排\n",
    "        agent_n = len(agent_id_list) #智能体数量 #12为state_dim #3*12=36\n",
    "        #out_obs_list = [F.relu(fc_obs(obs)) for fc_obs, obs in zip(self.fc_obs, observation_batch_list)]\n",
    "        out_obs_list = [F.relu(self.fc_obs(x[:,:12])) , F.relu(self.fc_obs(x[:,12:24])) , F.relu(self.fc_obs(x[:,24:36]))]               \n",
    "        # out_obs_list shape: [(batch_size, hidden_dim), ...] #即 batch_size * hidden_dim * agent_count\n",
    "\n",
    "        #out_action_list = [F.relu(fc_action(action)) for fc_action, action in zip(self.fc_action, action_batch_list)]\n",
    "        out_action_list = [F.relu(self.fc_action(x[:,36:37])) , F.relu(self.fc_action(x[:,37:38])) , F.relu(self.fc_action(x[:,38:39]))]\n",
    "        # out_action_list shape: [(batch_size, hidden_dim), ...]\n",
    "\n",
    "        #qvalue_list = []\n",
    "        #for i in range(self.args.agent_count):\n",
    "        encoder_input = torch.cat(out_obs_list + [out_action_list[agent_id_index]], dim=1)\n",
    "        # encoder_input shape: (batch_size, hidden_dim * (agent_count + 1))\n",
    "\n",
    "        decoder_input = torch.cat(out_action_list[:agent_id_index] + out_action_list[agent_id_index+1:], dim=1)\n",
    "        # decoder_input shape: (batch_size, hidden_dim * (agent_count - 1))\n",
    "\n",
    "        contextual_vector = self.attention_modules(encoder_input, decoder_input)\n",
    "        # contextual_vector shape: (batch_size, hidden_dim)\n",
    "\n",
    "        qvalue = self.fc_qvalue(contextual_vector)\n",
    "            # qvalue shape: (batch_size, 1)\n",
    "\n",
    "            #qvalue_list.append(qvalue)\n",
    "\n",
    "        return qvalue\n",
    "\n",
    "# 例子\n",
    "args_dict = {'agent_count': 2, 'observation_dim_list': [3, 3], 'hidden_dim': 4, 'action_dim_list': [2, 2], 'head_count': 3, 'agent_name': 'qita'}\n",
    "args = argparse.Namespace(**args_dict)\n",
    "critic = CriticAttentionalMADDPG(args)\n",
    "observation_batch_list = [torch.randn(2, 3), torch.randn(2, 3)]\n",
    "action_batch_list = [torch.randn(2, 2), torch.randn(2, 2)]\n",
    "output = critic(observation_batch_list, action_batch_list)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1130],\n",
      "        [ 1.0746],\n",
      "        [-1.1199],\n",
      "        [ 2.1931],\n",
      "        [ 1.7309],\n",
      "        [ 2.0937],\n",
      "        [ 1.9863],\n",
      "        [-1.7342],\n",
      "        [-1.5999],\n",
      "        [-1.7113],\n",
      "        [-3.7277],\n",
      "        [-1.9056]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([12, 1])\n"
     ]
    }
   ],
   "source": [
    "class MLPNetwork1(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim_1=256, hidden_dim_2=128,non_linear=nn.ReLU()):\n",
    "        super(MLPNetwork1, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_dim, hidden_dim_1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim_2, out_dim)\n",
    "        \n",
    "        #根据计算增益\n",
    "        gain1 = nn.init.calculate_gain('relu')\n",
    "        #Xavier均匀分布初始化\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight, gain=gain1)\n",
    "        #初始化参数\n",
    "        self.fc1.bias.data.fill_(0.01)\n",
    "        self.fc2.bias.data.fill_(0.01)\n",
    "        self.fc3.bias.data.fill_(0.01)\n",
    "    def forward(self, x): #输入维度：batch_size * in_dim 输出维度：batch_size * out_dim\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "# 例子\n",
    "global_obs_dim = 128  # 示例输入维度\n",
    "critic = MLPNetwork1(global_obs_dim, 1)\n",
    "#print(critic)\n",
    "x = torch.randn(12, 128) # 12个样本，128个维度\n",
    "output = critic(x)\n",
    "print(output) #输出\n",
    "print(output.shape) #输出维度 torch.Size([12, 1]) BATCH_SIZE * 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.4956],\n",
      "        [ 0.8701],\n",
      "        [-1.1959],\n",
      "        [ 2.1783],\n",
      "        [-2.8783],\n",
      "        [-1.7185],\n",
      "        [ 3.4321],\n",
      "        [-0.6630],\n",
      "        [-0.8066],\n",
      "        [ 1.5690],\n",
      "        [-2.6908],\n",
      "        [-0.2538]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MLPNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim_1=256, hidden_dim_2=128,non_linear=nn.ReLU()):\n",
    "        super(MLPNetwork, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim_1),\n",
    "            non_linear,\n",
    "            nn.Linear(hidden_dim_1, hidden_dim_2),\n",
    "            non_linear,\n",
    "            nn.Linear(hidden_dim_2, out_dim),\n",
    "        ).apply(self.init) #apply(self.init)是在初始化模块的权重和偏置时调用init方法\n",
    "\n",
    "    @staticmethod\n",
    "    def init(m):\n",
    "        \"\"\"init parameter of the module\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu') #zh-cn:计算增益\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight, gain=gain)#这行代码使用 Xavier 均匀分布初始化方法来初始化模块的权重（m.weight）。Xavier 初始化方法旨在使得网络各层的激活值和梯度的方差在传播过程中保持一致，有助于加速网络的收敛。gain 参数是根据 ReLU 激活函数的特性调整的。\n",
    "            m.bias.data.fill_(0.01) #zh-cn:这行代码使用常数 0.01 来初始化模块的偏置（m.bias）。\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "torch.manual_seed(1)\n",
    "# 例子\n",
    "global_obs_dim = 128  # 示例输入维度\n",
    "critic = MLPNetwork(global_obs_dim, 1)\n",
    "#print(critic)\n",
    "x = torch.randn(12, 128) # 12个样本，128个维度\n",
    "output = critic(x)\n",
    "print(output) #输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.4956],\n",
      "        [ 0.8701],\n",
      "        [-1.1959],\n",
      "        [ 2.1783],\n",
      "        [-2.8783],\n",
      "        [-1.7185],\n",
      "        [ 3.4321],\n",
      "        [-0.6630],\n",
      "        [-0.8066],\n",
      "        [ 1.5690],\n",
      "        [-2.6908],\n",
      "        [-0.2538]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 和上面一样 但不同实现\n",
    "class MLPNetwork1(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim_1=256, hidden_dim_2=128,non_linear=nn.ReLU()):\n",
    "        super(MLPNetwork1, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_dim, hidden_dim_1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim_2, out_dim)\n",
    "        \n",
    "        #根据计算增益\n",
    "        gain1 = nn.init.calculate_gain('relu')\n",
    "        #Xavier均匀分布初始化\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight, gain=gain1)\n",
    "        #初始化参数\n",
    "        self.fc1.bias.data.fill_(0.01)\n",
    "        self.fc2.bias.data.fill_(0.01)\n",
    "        self.fc3.bias.data.fill_(0.01)\n",
    "    def forward(self, x): #输入维度：batch_size * in_dim 输出维度：batch_size * out_dim\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "# 例子\n",
    "global_obs_dim = 128  # 示例输入维度\n",
    "critic = MLPNetwork(global_obs_dim, 1)\n",
    "#print(critic)\n",
    "x = torch.randn(12, 128) # 12个样本，128个维度\n",
    "output = critic(x)\n",
    "print(output) #输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 和上面一样 但不同实现\n",
    "class MLPNetwork1(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim_1=256, hidden_dim_2=128,non_linear=nn.ReLU()):\n",
    "        super(MLPNetwork1, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_dim, hidden_dim_1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim_2, out_dim)\n",
    "        \n",
    "        #根据计算增益\n",
    "        gain1 = nn.init.calculate_gain('relu')\n",
    "        #Xavier均匀分布初始化\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight, gain=gain1)\n",
    "        #初始化参数\n",
    "        self.fc1.bias.data.fill_(0.01)\n",
    "        self.fc2.bias.data.fill_(0.01)\n",
    "        self.fc3.bias.data.fill_(0.01)\n",
    "    def forward(self, x): #输入维度：batch_size * in_dim 输出维度：batch_size * out_dim\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "## 注意力机制改1\n",
    "class Attention1(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(Attention1, self).__init__()\n",
    "        self.query = nn.Linear(in_dim, hidden_dim, bias = False) #查询\n",
    "        self.key = nn.Linear(in_dim, hidden_dim, bias = False) #false 好\n",
    "        #self.value = nn.Linear(in_dim, hidden_dim)\n",
    "        self.value = nn.Sequential(nn.Linear(in_dim,hidden_dim),nn.LeakyReLU()) # 输出经过激活函数处理\n",
    "    \n",
    "    \n",
    "    def forward(self, e_q, e_k):  \n",
    "        '''\n",
    "        公式为：Attention(Q, K, V) = softmax(Q*K^T/sqrt(d_k)) * V 输出为当前智能体的注意力值\n",
    "        Q: 查询\n",
    "        K: 键\n",
    "        V: 值\n",
    "        d_k: 键的维度 这里用hidden_dim表示 即:K[0].shape[1]\n",
    "        e_q: 为batch_size * 1 * in_dim #e_q 为当前状态编码 或者输入batch_size  * in_dim 也可以 view有调整维度的功能\n",
    "        e_k: 为batch_size * n * in_dim n为【其余】智能体数量 #e_k为其余智能体状态编码\n",
    "        本质：在其余智能体中找到与当前智能体最相关的智能体\n",
    "        '''\n",
    "        Q = self.query(e_q)  #查询当前智能体价值 Q: batch_size * hidden_dim\n",
    "        K = self.key(e_k)  #其余智能体的键 K: batch_size * n * hidden_dim\n",
    "        V = self.value(e_k) #其余智能体的值 V: batch_size * n * hidden_dim\n",
    "        d_k = K[0].shape[1] #键的维度\n",
    "        '''\n",
    "        Q -> batch_size * 1 * hidden_dim \n",
    "        K -> batch_size * hidden_dim * n\n",
    "        Q*K^T -> batch_size * 1 * n\n",
    "        '''\n",
    "        fenzi = torch.matmul(Q.view(Q.shape[0], 1, -1),K.permute(0, 2, 1)) #Q*K^T\n",
    "        atten_scores = fenzi/np.sqrt(d_k) # 维度为 batch_size * 1 * n\n",
    "        atten_weights = torch.softmax(atten_scores, dim=2) # 维度为 batch_size * 1 * n\n",
    "        '''\n",
    "        V -> batch_size * hidden_dim * n\n",
    "        atten_weights : batch_size * 1 * n  #其余智能体的权重值\n",
    "        V.permute(0, 2, 1) * atten_weights) : batch_size * hidden_dim * n\n",
    "        atten_values : batch_size * hidden_dim  #加权求和表示当前智能体的注意力值\n",
    "        '''\n",
    "        atten_values = (V.permute(0, 2, 1) * atten_weights).sum(dim=2)\n",
    "\n",
    "        return atten_values #当前智能体的注意力值\n",
    "\n",
    "class MLPNetworkWithAttention1(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim_1=256, hidden_dim_2=128, attention_dim=256, non_linear=nn.ReLU()):\n",
    "        '''\n",
    "        # in_dim: 为所有智能体状态和动作维度之和 这里是13*3=39 #这里似乎没用到\n",
    "        # 输入维度为 batch_size * in_dim 输出维度为 batch_size * out_dim\n",
    "        注意力机制作用：改善了MADDPG中critic输入随智能体数目增大而指数增加的扩展性问题\n",
    "        '''\n",
    "        super(MLPNetworkWithAttention1, self).__init__()\n",
    "        self.attention = Attention1(attention_dim, attention_dim)\n",
    "        self.fc1 = torch.nn.Linear(2*attention_dim, hidden_dim_1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim_2, out_dim)\n",
    "        \n",
    "        # 根据计算增益\n",
    "        gain1 = nn.init.calculate_gain('relu')\n",
    "        # Xavier均匀分布初始化\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight, gain=gain1)\n",
    "        # 初始化参数\n",
    "        self.fc1.bias.data.fill_(0.01)\n",
    "        self.fc2.bias.data.fill_(0.01)\n",
    "        self.fc3.bias.data.fill_(0.01)\n",
    "\n",
    "        # 注意力相关\n",
    "        self.embedding = nn.Linear(13, attention_dim) #13 为状态维度12+动作维度1\n",
    "        self.in_fn = nn.BatchNorm1d(2*attention_dim) # BatchNorm1d 只是对每个样本的特征维度归一化 输出和输入维度一样\n",
    "        self.in_fn.weight.data.fill_(1) #确保在训练开始时，批归一化层不会对输入数据进行任何不必要的缩放和平移，从而保持输入数据的原始分布。这有助于稳定训练过程。\n",
    "        self.in_fn.bias.data.fill_(0) \n",
    "    \n",
    "    def forward(self, x ,agent_id,agents): # x本来为cat后的张量,增加 x,agent_i,agents #agents=Agent() \n",
    "        agent_id_list = list(agents.keys())\n",
    "        agent_id_index = agent_id_list.index(agent_id) #获取agent_id在agents中的索引 按照顺序排\n",
    "        agent_n = len(agent_id_list) #智能体数量 #12为state_dim #3*12=36\n",
    "        '''\n",
    "        temp1 : permute前 :   1 * batch_size * 12 permute后 : batch_size * 1 * 12\n",
    "        temp2 : x[:, 36 + agent_id_index] : batch_size\n",
    "        permute前 :  1 * 1 * batch_size   permute后 : batch_size * 1 * 1\n",
    "        torch.cat((temp1,temp2),2) : batch_size * 1 * 13\n",
    "        e_q : batch_size * 1 * attention_dim\n",
    "        【注】我这里动作为列表不是离散值\n",
    "        '''\n",
    "        #print('x:',x[:,12:24].shape)\n",
    "        temp1 = torch.unsqueeze(x[:,12 * agent_id_index:12 * agent_id_index + 12],0).permute(1, 0, 2) #.permute(1, 0, 2)将第0维和第1维进行交换\n",
    "        temp2 = torch.unsqueeze(torch.unsqueeze(x[:, 36 + agent_id_index], 0),0).permute(2, 1, 0)  ######\n",
    "        e_q = self.embedding(torch.cat((temp1,temp2),2))\n",
    "        '''\n",
    "        n :【其余】智能体数量\n",
    "        torch.cat((temp3,temp4),2) : batch_size * 1 * 13\n",
    "        embedding(torch.cat((temp3,temp4),2)) : batch_size * 1 * attention_dim\n",
    "        stack: n * batch_size * 1 * attention_dim  #堆叠 : 将多个张量堆叠在一起\n",
    "        squeeze: n * batch_size * attention_dim #压缩 : 去掉维度为1的维度      \n",
    "        e_k : batch_size * n * attention_dim \n",
    "        '''\n",
    "        e_k = []\n",
    "        for j in range(agent_n): # 其余智能体\n",
    "            if j!=agent_id_index:\n",
    "                temp3 = torch.unsqueeze(x[:,12 * j:12 * j + 12],0).permute(1, 0, 2)\n",
    "                temp4 = torch.unsqueeze(torch.unsqueeze(x[:, 36 + j], 0), 0).permute(2, 1, 0)\n",
    "                e_k.append(agents[agent_id_list[j]].critic.embedding(torch.cat((temp3,temp4),2)))   #agents[j].critic.embedding 在这里使用集中式训练的critic,所以其实这里embedding是一样的\n",
    "\n",
    "        e_k_s  = torch.squeeze(torch.stack(e_k))  \n",
    "        e_k = e_k_s.permute(1,0,2)\n",
    "\n",
    "        atten_values = self.attention(e_q, e_k) #输出 batch_size * attention_dim\n",
    "        X_in=torch.cat([torch.squeeze(e_q), atten_values], dim=1) # 输出 batch_size * (attention_dim*2)\n",
    "        h1 = F.relu(self.fc1(self.in_fn(X_in)))\n",
    "        h2 = F.relu(self.fc2(h1))      \n",
    "        out = (self.fc3(h2))\n",
    "\n",
    "        return out #输出 batch_size * out_dim\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
