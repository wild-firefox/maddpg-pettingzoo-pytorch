{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations: {'adversary_0': array([-0.32956174, -1.108241  , -0.7522789 ,  0.12979814, -0.35218278,\n",
      "       -1.3779373 , -0.8661719 , -1.4783411 ], dtype=float32), 'agent_0': array([-0.40009615,  1.5077355 ,  0.02262103,  0.26969635, -0.40009615,\n",
      "        1.5077355 ,  0.35218278,  1.3779373 , -0.51398915, -0.10040382],\n",
      "      dtype=float32), 'agent_1': array([0.11389301, 1.6081393 , 0.5366102 , 0.37010017, 0.11389301,\n",
      "       1.6081393 , 0.8661719 , 1.4783411 , 0.51398915, 0.10040382],\n",
      "      dtype=float32)}\n",
      "infos: {'adversary_0': {}, 'agent_0': {}, 'agent_1': {}}\n",
      "agents: ['adversary_0', 'agent_0', 'agent_1']\n",
      "action: {'adversary_0': 4, 'agent_0': 2, 'agent_1': 1}\n",
      "observations: {'adversary_0': array([-0.32956174, -1.108241  , -0.7522789 ,  0.12979814, -0.35218278,\n",
      "       -1.3779373 , -0.8661719 , -1.4783411 ], dtype=float32), 'agent_0': array([-0.40009615,  1.5077355 ,  0.02262103,  0.26969635, -0.40009615,\n",
      "        1.5077355 ,  0.35218278,  1.3779373 , -0.51398915, -0.10040382],\n",
      "      dtype=float32), 'agent_1': array([0.11389301, 1.6081393 , 0.5366102 , 0.37010017, 0.11389301,\n",
      "       1.6081393 , 0.8661719 , 1.4783411 , 0.51398915, 0.10040382],\n",
      "      dtype=float32)}\n",
      "rewards: defaultdict(<class 'int'>, {'adversary_0': -0.7633944726975814, 'agent_0': -0.796523208122647, 'agent_1': -0.796523208122647})\n",
      "terminations: {'adversary_0': False, 'agent_0': False, 'agent_1': False}\n",
      "truncations: {'adversary_0': False, 'agent_0': False, 'agent_1': False}\n",
      "infos: {'adversary_0': {}, 'agent_0': {}, 'agent_1': {}}\n",
      "state_dim: 10\n",
      "action_dim: 5\n",
      "max_num_agents: 3\n",
      "n_agents: 3\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.mpe import simple_adversary_v3\n",
    "\n",
    "env = simple_adversary_v3.parallel_env(render_mode=\"human\")\n",
    "observations, infos = env.reset()\n",
    "print('observations:',observations)\n",
    "print('infos:',infos)\n",
    "print('agents:',env.agents)\n",
    "action = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "print('action:',action)\n",
    "observations, rewards, terminations, truncations, infos = env.step(action)\n",
    "print('observations:',observations)\n",
    "print('rewards:',rewards)\n",
    "print('terminations:',terminations)\n",
    "print('truncations:',truncations)\n",
    "print('infos:',infos)\n",
    "env.close()\n",
    "print('state_dim:',env.observation_space('agent_0').shape[0])\n",
    "print('action_dim:',env.action_space('agent_0').n)   #env.action_spaces['agent_0'].n) 是个字典\n",
    "print('max_num_agents:',env.max_num_agents)\n",
    "print('n_agents:',env.num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adversary_0': 1, 'agent_0': 0, 'agent_1': 1}\n"
     ]
    }
   ],
   "source": [
    "from MADDPG import MADDPG\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_adversary_v3, simple_spread_v3, simple_tag_v3\n",
    "def get_env(env_name, ep_len=25):\n",
    "    \"\"\"create environment and get observation and action dimension of each agent in this environment\"\"\"\n",
    "    new_env = None\n",
    "    if env_name == 'simple_adversary_v3':\n",
    "        new_env = simple_adversary_v3.parallel_env(max_cycles=ep_len,render_mode='rgb_array')\n",
    "    if env_name == 'simple_spread_v3':\n",
    "        new_env = simple_spread_v3.parallel_env(max_cycles=ep_len)\n",
    "    if env_name == 'simple_tag_v3':\n",
    "        new_env = simple_tag_v3.parallel_env(max_cycles=ep_len)\n",
    "\n",
    "    new_env.reset()\n",
    "    _dim_info = {}\n",
    "    for agent_id in new_env.agents:\n",
    "        _dim_info[agent_id] = []  # [obs_dim, act_dim]\n",
    "        _dim_info[agent_id].append(new_env.observation_space(agent_id).shape[0])\n",
    "        _dim_info[agent_id].append(new_env.action_space(agent_id).n)\n",
    "\n",
    "    return new_env, _dim_info\n",
    "dim_info = get_env('simple_adversary_v3')[1]\n",
    "#dim_info, capacity, batch_size, actor_lr, critic_lr, res_dir)\n",
    "maddpg = MADDPG(dim_info,1,36,0.01,0.01,'test')\n",
    "\n",
    "print(maddpg.select_action(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'adversary_0': -0.2605955550976603, 'agent_0': -0.8693780243973747, 'agent_1': -0.8693780243973747})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 创建一个 defaultdict，默认值类型为 int\n",
    "dd = defaultdict(int)\n",
    "\n",
    "# 向 defaultdict 中添加键值对\n",
    "dd['adversary_0'] = -0.2605955550976603\n",
    "dd['agent_0'] = -0.8693780243973747\n",
    "dd['agent_1'] = -0.8693780243973747\n",
    "\n",
    "# 打印 defaultdict\n",
    "print(dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<AddBackward0>)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "from Agent import Agent\n",
    "import torch\n",
    "\n",
    "agent = Agent(12,1,36,0.01,0.01)\n",
    "obs = torch.tensor([1,2,3,4,5,6,7,8,9,10,11,12],dtype=torch.float32)\n",
    "print(agent.action(obs))\n",
    "print(agent.target_action(obs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "6.123233995736766e-17\n",
      "-1.0\n",
      "-1.8369701987210297e-16\n",
      "1.0\n",
      "0.0007963267107332633\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(math.cos(0))\n",
    "print(math.cos(math.pi/2))\n",
    "print(math.cos(math.pi))\n",
    "print(math.cos(3*math.pi/2))\n",
    "print(math.cos(2*math.pi))\n",
    "\n",
    "print(math.tanh(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.05185142]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.34829525, -1.38448912])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "action =1\n",
    "action = action + 0.1* np.random.randn() # np.random.randn(1) 1有增加array维度的作用    \n",
    "print(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7499], grad_fn=<TanhBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0958])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128,):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "        '''\n",
    "        #根据计算增益\n",
    "        gain1 = nn.init.calculate_gain('relu')\n",
    "        gain2 = nn.init.calculate_gain('tanh')\n",
    "        #Xavier均匀分布初始化\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight, gain=gain1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight, gain=gain2)\n",
    "        #初始化参数\n",
    "        #self.fc1.weight.data.normal_(0, 0.1)\n",
    "        self.fc1.bias.data.fill_(0.01)\n",
    "        #self.fc2.weight.data.normal_(0, 0.1)\n",
    "        self.fc2.bias.data.fill_(0.01)\n",
    "\n",
    "        #self.action_bound = action_bound  # action_bound是环境可以接受的动作最大值\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return torch.tanh(self.fc2(x)) # tanh输出范围是(-1,1)，符合动作的范围 还需要后续clip\n",
    "policy = PolicyNet(12,1)\n",
    "obs = torch.tensor([1,2,3,4,5,6,7,8,9,10,11,12],dtype=torch.float32)\n",
    "print(policy(obs))\n",
    "torch.randn(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1]\n",
      "[[1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "win_list = [1,0,0,0,0,0,0,0,0,0]\n",
    "win_array = np.mean(np.array(win_list).reshape(-1, 10), axis=1) #axis=1表示按行求均值\n",
    "print(win_array)\n",
    "print(np.array(win_list).reshape(-1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "done ={'Red-0': False, 'Red-1': False, 'Red-2': False}\n",
    "print(any(done.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards': {'Red-0': array([ -4.        ,  -6.64236263,  -6.39353434, ..., 183.4901285 ,\n",
      "       184.23064647, 188.91      ]), 'Red-1': array([128.31      ,  -5.24634945,  -6.43049854, ..., 185.66645085,\n",
      "       194.87857453, 166.50867731]), 'Red-2': array([ -4.        ,  -4.02393492,  -8.36051615, ..., 187.9       ,\n",
      "        14.18      , 187.9       ])}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 假设你有一个名为 'data.pkl' 的文件\n",
    "with open('results\\\\toy_env\\\\24\\\\rewards.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# 现在 'data' 变量包含了从 .pkl 文件中恢复的对象\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m angle\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3.00596919\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m angle \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(angle)\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "angle=[3.00596919]\n",
    "assert 0 <= angle <= 2 * math.pi\n",
    "print(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "red0_hp_l = [0]\n",
    "print(np.mean(red0_hp_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 // 4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
